{"meta":{"title":"Snail Book","subtitle":"Snail 的知识库","description":"个人文档","author":"Sanil","url":"https://champion-yang.github.io","root":"/"},"pages":[{"title":"about","date":"2024-06-28T05:33:28.000Z","updated":"2024-07-04T08:26:54.615Z","comments":false,"path":"about/index.html","permalink":"https://champion-yang.github.io/about/index.html","excerpt":"","text":"Hi there关于我 目前在 Nsfocus 任研发工程师. 曾先后在某金融公司担任风控模型研发工程师, 某人工智能公司担任 NLP 模型研发工程师 热爱编程 系统架构师【备考中】 授权相关专利 1 篇 曾获电子设计大赛三等奖, 全国大学生计算机设计大赛二等奖 懂一些项目管理 PMP 证书 信息系统与项目管理师【未获得… 论文被卡】 联系我QQ:529352969WeXin: Xiaobei__1024__"},{"title":"lang","date":"2024-06-28T05:51:04.000Z","updated":"2024-06-28T05:51:04.792Z","comments":true,"path":"lang/index.html","permalink":"https://champion-yang.github.io/lang/index.html","excerpt":"","text":""},{"title":"tags","date":"2024-06-28T05:34:11.000Z","updated":"2024-07-04T03:26:40.068Z","comments":false,"path":"tags/index.html","permalink":"https://champion-yang.github.io/tags/index.html","excerpt":"","text":""},{"title":"","date":"2024-07-04T03:10:47.430Z","updated":"2024-07-04T03:10:47.430Z","comments":true,"path":"css/book.css","permalink":"https://champion-yang.github.io/css/book.css","excerpt":"","text":"div.info { display: flex; flex-wrap: wrap; padding: 1rem; } div.info div.image { flex: 1 1 200px; display: flex; align-items: center; } div.info div.image img { width: 60%; } div.info div.info-card { flex: 3 1 500px; }"},{"title":"","date":"2024-07-04T03:13:42.687Z","updated":"2024-07-04T03:13:42.687Z","comments":true,"path":"books/index.html","permalink":"https://champion-yang.github.io/books/index.html","excerpt":"","text":"《无职转生》 作者：理不尽な孫の手 出版时间： 2014-01-24——2022-11-25 推荐指数： 34岁无职童贞尼特身无分文地被赶出家门，发现自己的人生已经完全走投无路。刚刚产生后悔的想法，他就被卡车撞死了。然后醒来的地方居然是——剑与魔法的异世界！！重生为名叫卢迪乌斯的婴儿的他下定决心，“这次一定要认真地活下去……！”，一定要度过一段不会后悔的人生。他利用前世的智力很快使得自己的魔法的才能开花结果，结果一位年轻的女孩子成了自己的家庭教师。并且又与一位有着绿宝石般美丽秀发的四分一血统的精灵相遇。他崭新的人生开始前进。——让人憧憬的转生型奇幻小说，在这里开始。"},{"title":"categories","date":"2024-07-04T03:22:40.000Z","updated":"2024-07-04T03:25:25.064Z","comments":false,"path":"categories/index.html","permalink":"https://champion-yang.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"2024-07-18-api测试思路及体系构建","slug":"06_其他/2024-07-18-api测试思路及体系构建","date":"2024-07-18T02:32:37.000Z","updated":"2024-07-18T02:32:37.559Z","comments":true,"path":"2024/07/18/06_其他/2024-07-18-api测试思路及体系构建/","permalink":"https://champion-yang.github.io/2024/07/18/06_%E5%85%B6%E4%BB%96/2024-07-18-api%E6%B5%8B%E8%AF%95%E6%80%9D%E8%B7%AF%E5%8F%8A%E4%BD%93%E7%B3%BB%E6%9E%84%E5%BB%BA/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"02信息技术发展","slug":"05_软考/01_中项/02信息技术发展","date":"2024-07-17T03:21:35.000Z","updated":"2024-07-17T03:25:00.750Z","comments":true,"path":"2024/07/17/05_软考/01_中项/02信息技术发展/","permalink":"https://champion-yang.github.io/2024/07/17/05_%E8%BD%AF%E8%80%83/01_%E4%B8%AD%E9%A1%B9/02%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95/","excerpt":"","text":"信息技术发展信息技术及其发展","categories":[{"name":"软考","slug":"软考","permalink":"https://champion-yang.github.io/categories/%E8%BD%AF%E8%80%83/"},{"name":"中项","slug":"软考/中项","permalink":"https://champion-yang.github.io/categories/%E8%BD%AF%E8%80%83/%E4%B8%AD%E9%A1%B9/"}],"tags":[{"name":"软考","slug":"软考","permalink":"https://champion-yang.github.io/tags/%E8%BD%AF%E8%80%83/"}]},{"title":"01信息化发展","slug":"05_软考/01_中项/01信息化发展","date":"2024-07-16T08:48:44.000Z","updated":"2024-07-16T10:49:58.342Z","comments":true,"path":"2024/07/16/05_软考/01_中项/01信息化发展/","permalink":"https://champion-yang.github.io/2024/07/16/05_%E8%BD%AF%E8%80%83/01_%E4%B8%AD%E9%A1%B9/01%E4%BF%A1%E6%81%AF%E5%8C%96%E5%8F%91%E5%B1%95/","excerpt":"","text":"信息化发展信息与信息化信息的特征客观、普遍、无限、动态、相对、依附、变换、传递、层次、系统、转化 信息的质量精确、完整、可报、及时、经济、可验证、安全 信息具有价值, 价值由质量决定 信息系统生命周期(类比软件生命周期)简化为: - 立项【规划】 - 开发【分析、设计、实施】 - 运维 - 消亡 系统规划(可行性分析与项目开发计划) 系统建设的必要性和可能性 可行性研究报告 系统设计任务书 系统建设方案 实施计划 系统分析(需求分析) 逻辑设计阶段, 输出系统说明书, 和用户确认需求的基础 系统设计和验收的依据 系统设计(概要设计、详细设计) 解决”怎么做”的问题, 物理模型 设计材料 系统实施(编码、测试) 系统运行和维护(维护) 国家信息化体系 内容 地位 速记 信息资源 核心任务 资源任务 信息网络 基础设施 网络基础 信息技术应用 龙头 应用龙头 信息技术和产业 物质基础 技术产业为物基 信息化人才 成功之本 人本 政策法规和标准规范 保障 规保 国家信息化体系(十四五)数据治理 密码区块链 信息互联互通 智能网联 网络安全 现代化基础设施新基建 分类 内容 特点 信息基础设施 通、新、算 技术新 融合基础设施 利用大数据、人工智能支持传统设施转型 应用新 创新技术设施 科研开发等公益属性 平台新 工业互联网网络为基础、平台为中枢、数据为要素、安全为保障 城市物联网智慧物流、交通、安防、能源环保、医疗、建筑、家居、零售 产业现代化农业农村现代化加强基础设施建设 发展智慧农业 建设数字乡村 工业现代化信息化和工业化结合 技术融合、产品融合、业务融合、产业衍生 智能制造 能力成熟度模型 级别 内容 特点 一级 规划 二级 规范 三级 集成 共享 四级 优化 预测 五级 引领 服务现代化消费互联网 媒体属性、产业属性 数字中国数字经济数字产业化 产业数字化 数字化治理 数据价值化 数字政府一网通办 跨省通办 一网通管 数字社会数字生态数字化转型与元宇宙元宇宙沉浸式体验 虚拟身份 虚拟经济 虚拟社会治理","categories":[{"name":"软考","slug":"软考","permalink":"https://champion-yang.github.io/categories/%E8%BD%AF%E8%80%83/"},{"name":"中项","slug":"软考/中项","permalink":"https://champion-yang.github.io/categories/%E8%BD%AF%E8%80%83/%E4%B8%AD%E9%A1%B9/"}],"tags":[{"name":"软考","slug":"软考","permalink":"https://champion-yang.github.io/tags/%E8%BD%AF%E8%80%83/"}]},{"title":"Elasticsearch 操作API","slug":"03_es/2024-07-11-escrud","date":"2024-07-11T07:55:29.000Z","updated":"2024-07-11T08:34:32.447Z","comments":true,"path":"2024/07/11/03_es/2024-07-11-escrud/","permalink":"https://champion-yang.github.io/2024/07/11/03_es/2024-07-11-escrud/","excerpt":"","text":"集群集群健康状态 GET /_cat/health?v 12345678910111213141516&#123; &quot;epoch&quot;: &quot;1720686430&quot;, &quot;timestamp&quot;: &quot;08:27:10&quot;, &quot;cluster&quot;: &quot;ISOP_1713495047759&quot;, &quot;status&quot;: &quot;yellow&quot;, &quot;node.total&quot;: &quot;1&quot;, &quot;node.data&quot;: &quot;1&quot;, &quot;shards&quot;: &quot;75&quot;, &quot;pri&quot;: &quot;75&quot;, &quot;relo&quot;: &quot;0&quot;, &quot;init&quot;: &quot;0&quot;, &quot;unassign&quot;: &quot;1&quot;, &quot;pending_tasks&quot;: &quot;0&quot;, &quot;max_task_wait_time&quot;: &quot;-&quot;, &quot;active_shards_percent&quot;: &quot;98.7%&quot;&#125; title title title conte conte conte 索引索引操作文档操作增删改查 https://www.elastic.co/guide/en/elasticsearch/reference/current/elasticsearch-intro.html","categories":[{"name":"大数据","slug":"大数据","permalink":"https://champion-yang.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"存储","slug":"大数据/存储","permalink":"https://champion-yang.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://champion-yang.github.io/tags/Elasticsearch/"}]},{"title":"Elasticsearch 相关概念","slug":"03_es/2024-07-10-es概念相关","date":"2024-07-10T07:55:29.000Z","updated":"2024-07-11T03:22:58.583Z","comments":true,"path":"2024/07/10/03_es/2024-07-10-es概念相关/","permalink":"https://champion-yang.github.io/2024/07/10/03_es/2024-07-10-es%E6%A6%82%E5%BF%B5%E7%9B%B8%E5%85%B3/","excerpt":"","text":"ES 是什么、干什么用的 分布式的搜索引擎和数据分析引擎 搜索业务, 类比于 mysql, 可以进行数据查询, 包括全文检索, 结构化检索等. 还支持部分匹配搜索, 搜索纠错, 搜索推荐等; Elasticsearch作为传统数据库的一个补充, 提供了数据库所不不能提供的很多功能 数据分析业务, 通过对数据查询的结果进行分组聚合等操作, 完成数据分析的工作 对海量数据进行近实时的处理 分布式: ES自动可以将海量数据分散到多台服务器上去存储和检索 近实时: 检索个数据要花费1小时(这就不是近实时, 是离线批处理, batch-processing); 在秒级别对数据进行搜索和分析 将全文检索(lucene)、数据分析以及分布式技术合并到一起形成了 ES. 一些概念 Near Realtime(NRT)近实时: 从写入数据到数据可以被搜索到有一个小延迟(大概1秒); Cluster 集群: 通过网络, 利用软件将多台低成本的计算机的资源统一进行协调调度, 从而实现一台”超级计算机”, 由一个或多个节点组成 Node 节点: 单个 es 的实例, 通常一个节点运行在一个隔离的容器或虚拟机中 Document&amp;field 文档: Document相当于mysql中的一行, 是es中最小的数据单元; field 相当于mysql中的字段 Index 索引: 相当于mysql中的数据库 Type 类型: 一个 index 索引中可以存在多个 type, 可类比于关系型数据库中的”表”的概念, 但是这个使用的范围比较受限, 从表现形式来说, 他是在 doc 文档上通过 _type 字段来进行区分, 则不同 type 里的字段需要保持一致. 所以有说法是 _type 是 es 早期设计的缺陷. 在5.x以前的版本里边, 一个index下面是支持多个type的, 在6.x的版本里改为一个index只支持一个type, type可以自定义.7.x的版本所有的type默认为_doc(自定义type也能用, 但是会提示不推荐) shard 分片: 单台机器无法存储大量数据, es 可以将一个索引中的数据切分为多个 shard, 分布在多台服务器上存储. 有了 shard 就可以横向扩展, 存储更多数据, 让搜索和分析等操作分布到多台服务器上去执行, 提升吞吐量和性能. replica 复制集&#x2F;副本: 每个 shard 可以创建多个 replica副本.replica 可以在 shard 故障时提供备用服务, 保证数据不丢失, 多个 replica 还可以提升搜索操作的吞吐量和性能. luceneLucene 是一个开放源码的全文检索引擎工具包, 提供了完整的查询引擎和索引引擎, 部分语种文本分析引擎, 并不是一个完整的全文检索引擎, 仅提供了全文检索引擎架构, 可以作为一个工具包结合各类插件为项目提供部分高性能的全文检索功能; 现在常用的 ElasticSearch、Solr 等全文搜索引擎均是基于 Lucene 实现的. 倒排索引 倒排索引(英语: Inverted index), 也常被称为反向索引、置入档案或反向档案, 是一种索引方法, 被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射. 它是文档检索系统中最常用的数据结构.[维基百科] 正向索引: 正向索引是基于文档建立的, 它记录文档中每个单词的位置信息. 在正向索引中, 通过文档ID可以迅速找到文档中的所有单词及其位置. 正向索引的示例: 12文档1: [&quot;Elasticsearch&quot;, 位置1; &quot;is&quot;, 位置2; &quot;a&quot;, 位置3; …]文档2: [&quot;Elasticsearch&quot;, 位置1; &quot;allows&quot;, 位置2; &quot;you&quot;, 位置3; …] 倒排索引: 它基于单词(term)建立索引, 而不是基于文档. 这意味着, 对于文档中的每个单词, 倒排索引都会记录哪些文档包含该单词以及该单词在文档中的位置信息(通常是词频和位置). 在 ES 中内置了很多的分词器, 在保存数据后, 会对数据进行分析, 生成词典, 每个单词会指向一个或多个倒排列表 倒排索引的结构: 词典(Term Dictionary): 包含所有单词的列表, 每个单词指向一个或多个倒排列表. 倒排列表(Posting List): 对于每个单词, 包含一个列表, 其中记录了包含该单词的文档ID和该单词在文档中的位置信息. 如下有两个文档:文档1: “Elasticsearch is a powerful search engine.”文档2: “Elasticsearch allows you to store, search, and analyze data efficiently.” ES 会构建一个词典, 包括了上述所有的单词.每个单词会指向一个或多个倒排列表. Elasticsearch: [文档1的ID, 位置1; 文档2的ID, 位置1] Lucene 全文索引的核心是基于倒排索引实现的快速索引机制. https://zq99299.github.io/note-book/elasticsearch-core/https://cloud.tencent.com/developer/article/2393710","categories":[{"name":"大数据","slug":"大数据","permalink":"https://champion-yang.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"存储","slug":"大数据/存储","permalink":"https://champion-yang.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://champion-yang.github.io/tags/Elasticsearch/"}]},{"title":"kafka 保证数据一致性","slug":"02_kafka/2027-07-09kafka05-幂等","date":"2024-07-09T08:57:15.000Z","updated":"2024-07-09T09:53:03.623Z","comments":true,"path":"2024/07/09/02_kafka/2027-07-09kafka05-幂等/","permalink":"https://champion-yang.github.io/2024/07/09/02_kafka/2027-07-09kafka05-%E5%B9%82%E7%AD%89/","excerpt":"","text":"消息系统语义概述(Overview of messaging system semantics)在一个分布式发布订阅消息系统中, 组成系统的计算机总会由于各自的故障而不能工作. 在Kafka中, 一个单独的broker, 可能会在生产者发送消息到一个topic的时候宕机, 或者出现网络故障, 从而导致生产者发送消息失败. 根据生产者如何处理这样的失败, 产生了不同的语义: 至少一次语义(At least once semantics): 如果生产者收到了Kafka broker的确认(acknowledgement, ack), 并且生产者的acks配置项设置为all(或-1), 这就意味着消息已经被精确一次写入Kafka topic了. 然而, 如果生产者接收ack超时或者收到了错误, 它就会认为消息没有写入Kafka topic而尝试重新发送消息. 如果broker恰好在消息已经成功写入Kafka topic后, 发送ack前, 出了故障, 生产者的重试机制就会导致这条消息被写入Kafka两次, 从而导致同样的消息会被消费者消费不止一次. 每个人都喜欢一个兴高采烈的给予者, 但是这种方式会导致重复的工作和错误的结果. 至多一次语义(At most once semantics): 如果生产者在ack超时或者返回错误的时候不重试发送消息, 那么消息有可能最终并没有写入Kafka topic中, 因此也就不会被消费者消费到. 但是为了避免重复处理的可能性, 我们接受有些消息可能被遗漏处理. 精确一次语义(Exactly once semantics): 即使生产者重试发送消息, 也只会让消息被发送给消费者一次. 精确一次语义是最令人满意的保证, 但也是最难理解的. 因为它需要消息系统本身和生产消息的应用程序还有消费消息的应用程序一起合作. 比如, 在成功消费一条消息后, 你又把消费的offset重置到之前的某个offset位置, 那么你将收到从那个offset到最新的offset之间的所有消息. 这解释了为什么消息系统和客户端程序必须合作来保证精确一次语义. 总结将服务器的ACK级别设置为-1, 可以保证Producer到Server之间不会丢失数据, 即At Least Once语义.相对的, 将服务器ACK级别设置为0, 可以保证生产者每条消息只会被发送一次, 即At Most Once语义. At Least Once可以保证数据不丢失, 但是不能保证数据不重复;相对的, At Least Once可以保证数据不重复, 但是不能保证数据不丢失. 但是, 对于一些非常重要的信息, 比如说交易数据, 下游数据消费者要求数据既不重复也不丢失, 即Exactly Once语义.在0.11版本以前的Kafka, 对此是无能为力的, 只能保证数据不丢失, 再在下游消费者对数据做全局去重. 对于多个下游应用的情况, 每个都需要单独做全局去重, 这就对性能造成了很大影响. 0.11版本的Kafka, 引入了一项重大特性:幂等性. 开启幂等性 enable.idempotence=true . 所谓的幂等性就是指Producer不论向Server发送多少次重复数据, Server端都只会持久化一条. 幂等性结合At Least Once语义, 就构成了Kafka的Exactly Once语义. 即: At Least Once + 幂等性 = Exactly Once Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游. 开启幂等性的Producer在初始化的时候会被分配一个PID, 发往同一Partition的消息会附带Sequence Number. 而Broker端会对&lt;PID, Partition, SeqNumber&gt;做缓存, 当具有相同主键的消息提交时, Broker只会持久化一条. 但是PID重启就会变化, 同时不同的Partition也具有不同主键, 所以幂等性无法保证跨分区跨会话的 Exactly Once. 所以0.11版本的Kafka引入了事务的概念 事务: 跨partition的原子性写操作Kafka现在支持使用新事务API原子性的对跨partition进行写操作, 该API允许producer发送批量消息到多个partition. 该功能同样支持在同一个事务中提交消费者offsets, 因此真正意义上实现了end-to-end的exactly-once delivery语义. 引用 https://www.cnblogs.com/luxiaoxun/p/13048474.html","categories":[{"name":"技术","slug":"技术","permalink":"https://champion-yang.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://champion-yang.github.io/tags/Kafka/"}]},{"title":"kafka的消费分区分配策略","slug":"02_kafka/2027-07-09kafka04-kafka的消费分区分配策略","date":"2024-07-09T08:33:19.000Z","updated":"2024-07-09T08:40:42.207Z","comments":true,"path":"2024/07/09/02_kafka/2027-07-09kafka04-kafka的消费分区分配策略/","permalink":"https://champion-yang.github.io/2024/07/09/02_kafka/2027-07-09kafka04-kafka%E7%9A%84%E6%B6%88%E8%B4%B9%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/","excerpt":"","text":"这将是一篇比较无聊的文章 一个consumer group中有多个consumer, 一个topic有多个partition, 所以必然会涉及到partition的分配问题, 即确定那个partition由哪个consumer来消费 Kafka有三种分配策略, 一是RoundRobin, 一是Range. 高版本还有一个StickyAssignor策略 将分区的所有权从一个消费者移到另一个消费者称为重新平衡(rebalance). 当以下事件发生时, Kafka 将会进行一次分区分配: 同一个 Consumer Group 内新增消费者. 消费者离开当前所属的Consumer Group, 包括shuts down或crashes. Range分区分配策略Range是对每个Topic而言的(即一个Topic一个Topic分), 首先对同一个Topic里面的分区按照序号进行排序, 并对消费者按照字母顺序进行排序. 然后用Partitions分区的个数除以消费者线程的总数来决定每个消费者线程消费几个分区. 如果除不尽, 那么前面几个消费者线程将会多消费一个分区. 假设n&#x3D;分区数&#x2F;消费者数量, m&#x3D;分区数%消费者数量, 那么前m个消费者每个分配n+1个分区, 后面的(消费者数量-m)个消费者每个分配n个分区. 假如有10个分区, 3个消费者线程, 把分区按照序号排列0, 1, 2, 3, 4, 5, 6, 7, 8, 9消费者线程为C1-0, C2-0, C2-1那么用partition数除以消费者线程的总数来决定每个消费者线程消费几个partition, 如果除不尽, 前面几个消费者将会多消费一个分区. 在我们的例子里面, 我们有10个分区, 3个消费者线程, 10&#x2F;3 &#x3D; 3, 而且除不尽, 那么消费者线程C1-0将会多消费一个分区, 所以最后分区分配的结果看起来是这样的:C1-0:0, 1, 2, 3C2-0:4, 5, 6C2-1:7, 8, 9如果有11个分区将会是:C1-0:0, 1, 2, 3C2-0:4, 5, 6, 7C2-1:8, 9, 10假如我们有两个主题T1, T2, 分别有10个分区, 最后的分配结果将会是这样:C1-0: T1(0, 1, 2, 3) T2(0, 1, 2, 3)C2-0: T1(4, 5, 6) T2(4, 5, 6)C2-1: T1(7, 8, 9) T2(7, 8, 9) RoundRobinAssignor分区分配策略RoundRobinAssignor策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序, 然后通过轮询方式逐个将分区以此分配给每个消费者. 使用RoundRobin策略有两个前提条件必须满足: 同一个消费者组里面的所有消费者的num.streams(消费者消费线程数)必须相等; 每个消费者订阅的主题必须相同. 加入按照 hashCode 排序完的topic-partitions组依次为T1-5, T1-3, T1-0, T1-8, T1-2, T1-1, T1-4, T1-7, T1-6, T1-9我们的消费者线程排序为C1-0, C1-1, C2-0, C2-1最后分区分配的结果为:C1-0 将消费 T1-5, T1-2, T1-6 分区C1-1 将消费 T1-3, T1-1, T1-9 分区C2-0 将消费 T1-0, T1-4 分区C2-1 将消费 T1-8, T1-7 分区 StickyAssignor分区分配策略Kafka从0.11.x版本开始引入这种分配策略, 它主要有两个目的:分区的分配要尽可能的均匀, 分配给消费者者的主题分区数最多相差一个分区的分配尽可能的与上次分配的保持相同. 当两者发生冲突时, 第一个目标优先于第二个目标. 鉴于这两个目的, StickyAssignor策略的具体实现要比RangeAssignor和RoundRobinAssignor这两种分配策略要复杂很多. 假设消费组内有3个消费者C0、C1、C2它们都订阅了4个主题:t0、t1、t2、t3并且每个主题有2个分区, 也就是说整个消费组订阅了t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1这8个分区最终的分配结果如下:消费者C0:t0p0、t1p1、t3p0消费者C1:t0p1、t2p0、t3p1消费者C2:t1p0、t2p1这样初看上去似乎与采用RoundRobinAssignor策略所分配的结果相同此时假设消费者C1脱离了消费组, 那么消费组就会执行再平衡操作, 进而消费分区会重新分配. 如果采用RoundRobinAssignor策略, 那么此时的分配结果如下:消费者C0:t0p0、t1p0、t2p0、t3p0消费者C2:t0p1、t1p1、t2p1、t3p1如分配结果所示, RoundRobinAssignor策略会按照消费者C0和C2进行重新轮询分配. 而如果此时使用的是StickyAssignor策略, 那么分配结果为:消费者C0:t0p0、t1p1、t3p0、t2p0消费者C2:t1p0、t2p1、t0p1、t3p1可以看到分配结果中保留了上一次分配中对于消费者C0和C2的所有分配结果, 并将原来消费者C1的”负担”分配给了剩余的两个消费者C0和C2, 最终C0和C2的分配还保持了均衡.如果发生分区重分配, 那么对于同一个分区而言有可能之前的消费者和新指派的消费者不是同一个, 对于之前消费者进行到一半的处理还要在新指派的消费者中再次复现一遍, 这显然很浪费系统资源.StickyAssignor策略如同其名称中的”sticky”一样, 让分配策略具备一定的”粘性”, 尽可能地让前后两次分配相同, 进而减少系统资源的损耗以及其它异常情况的发生. 到目前为止所分析的都是消费者的订阅信息都是相同的情况, 我们来看一下订阅信息不同的情况下的处理.举例, 同样消费组内有3个消费者:C0、C1、C2集群中有3个主题:t0、t1、t2这3个主题分别有1、2、3个分区也就是说集群中有t0p0、t1p0、t1p1、t2p0、t2p1、t2p2这6个分区消费者C0订阅了主题t0消费者C1订阅了主题t0和t1消费者C2订阅了主题t0、t1和t2如果此时采用RoundRobinAssignor策略:消费者C0:t0p0消费者C1:t1p0消费者C2:t1p1、t2p0、t2p1、t2p2如果此时采用的是StickyAssignor策略:消费者C0:t0p0消费者C1:t1p0、t1p1消费者C2:t2p0、t2p1、t2p2此时消费者C0脱离了消费组, 那么RoundRobinAssignor策略的分配结果为:消费者C1:t0p0、t1p1消费者C2:t1p0、t2p0、t2p1、t2p2StickyAssignor策略, 那么分配结果为:消费者C1:t1p0、t1p1、t0p0消费者C2:t2p0、t2p1、t2p2可以看到StickyAssignor策略保留了消费者C1和C2中原有的5个分区的分配:t1p0、t1p1、t2p0、t2p1、t2p2.从结果上看StickyAssignor策略比另外两者分配策略而言显得更加的优异, 这个策略的代码实现也是异常复杂. 引用 公众号: 大数据左右手","categories":[{"name":"技术","slug":"技术","permalink":"https://champion-yang.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://champion-yang.github.io/tags/Kafka/"}]},{"title":"kafka 常用命令","slug":"02_kafka/2024-07-08-kafka03命令","date":"2024-07-08T06:33:09.000Z","updated":"2024-07-09T07:02:07.205Z","comments":true,"path":"2024/07/08/02_kafka/2024-07-08-kafka03命令/","permalink":"https://champion-yang.github.io/2024/07/08/02_kafka/2024-07-08-kafka03%E5%91%BD%E4%BB%A4/","excerpt":"","text":"查看所有 topic ./kafka-topics.sh --zookeeper=10.5.208.13:2181 --list 创建topic——注意: 必须指定分区和副本, 否则会报错; 不同的版本需要指定的参数不同; Replication 不能超过 brokers 数量 bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 3 --topic testA bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 3 --partitions 3 --topic testA 删除topic ./kafka-topics.sh --zookeeper=10.5.208.13:2181 --topic xxx --delete 查看topic属性 ./kafka-topics.sh --describe --zookeeper=10.5.208.13:2181 --topic xxx topic增加分区 ./kafka-topics.sh --alter --zookeeper=10.5.208.13:2181 --topic xxx --partitions 10 分区数增加到10个, 如果topic的分区增加, 则分区逻辑或消息顺序将受到影响, 会引起rebalance操作 生产数据 ./kafka-console-producer.sh --broker-list localhost:9092 --topic TCP_6666 消费数据 ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first --from-beginning 重置偏移量 ./kafka-consumer-groups.sh --bootstrap-server=10.5.208.13:9092 --execute --reset-offsets --topic=xxx --group=testPlatform-local-pha --to-earliest 显示所有消费者 ./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list 查看Broker磁盘信息kafka-log-dirs.sh /home/master/kafka/bin/kafka-log-dirs.sh --bootstrap-server localhost:9092 --describe --topic-list vuln /home/master/kafka/bin/kafka-configs.sh --list --bootstrap-server localhost:9092 /home/master/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic vuln 查询 kafka 消费者组的信息 kafka=/home/master/kafka/ $kafka/bin/kafka-consumer-groups.sh --bootstrap-server=localhost:9092 --describe --group &quot;groupName&quot;","categories":[{"name":"技术","slug":"技术","permalink":"https://champion-yang.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://champion-yang.github.io/tags/Kafka/"}]},{"title":"python虚拟环境","slug":"python虚拟环境","date":"2024-07-08T06:14:22.000Z","updated":"2024-07-08T06:25:11.155Z","comments":true,"path":"2024/07/08/python虚拟环境/","permalink":"https://champion-yang.github.io/2024/07/08/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/","excerpt":"","text":"平常使用 anaconda 对开发环境做管理, 公司某天发了一条通知, 说 anaconda 未经授权不许使用, 存在 xxx 风险, 限令在一天内进行卸载. 在平常的一些 python 的项目中, anaconda 还是很方便的, 不同的项目管理不同的开发环境, 不过得合乎公司的规定, 所以只能通过其他的方式【命令】创建和管理虚拟环境了. 都是一些基础知识, 时间长没用, 做个记录. anaconda 使用 virtualenv 管理虚拟环境由于项目需要, 需要同时具备 py2 和 py3, 所以在创建虚拟环境的时候需要指定 python 解释器 创建虚拟环境 virtualenv env27 --python=python2.7 激活虚拟环境 windows: .\\env27\\Scripts\\activate 退出虚拟环境 windows: .\\env27\\Scripts\\deactivate","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://champion-yang.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://champion-yang.github.io/tags/Python/"}]},{"title":"kafka踩过的坑","slug":"02_kafka/2024-07-05-kafka02","date":"2024-07-05T08:48:17.000Z","updated":"2024-07-09T09:46:59.243Z","comments":true,"path":"2024/07/05/02_kafka/2024-07-05-kafka02/","permalink":"https://champion-yang.github.io/2024/07/05/02_kafka/2024-07-05-kafka02/","excerpt":"","text":"引用 我用kafka两年踩过的一些非比寻常的坑 https://www.szzdzhp.com/ python-kafka 在 python2 下测试 kafka-python==1.4.7 安装 pip2 install kafka-python==1.4.7 异常使用 kafka-python 产生的错误 ValueError: filedescriptor out of range in select() fd 超出 1024 导致的异常1 fd 超出 1024 导致的异常2 分析SimplePorducer send_messages接收多个发送消息, 用create_message_set打包成一个messages列表, messages传给ProduceRequestPayload创建一个payloads对象, 用payload对应的broker对payloads分组, 遍历这个分组每个分组发送一次, 用ProduceRequest对payload创建request对象, 用BrokerConnection对象发送request对象, 发送时使用的socket连接, 发送后返回一个future对象, 再用select.select监听这些连接, 当文件描述符超过1024时报错就是这时候发生的.client的创建类为KafkaClient, 基础类为SimpleClent 问题, select.select监听文件描述符报错, 文件描述符报错实际是在SimpleClent._send_broker_aware_request中产生, 可能的话可以重写_send_broker_aware_request方法 1234567from kafka import KafkaClient,SimpleProducerhost = &#x27;10.8.100.7:9092&#x27;client = KafkaClient(host=host)producer = SimpleProducer(client=client)messages = [&#x27;messageA&#x27;,&#x27;messageB&#x27;]topic = &#x27;topic&#x27;producer.send_messages(topic,*messages) KafkaProducer KafkaProducer通过send在代码逻辑上发送发送单条消息, 但是KafkaProducer实现了一个了一个线程, 通过send将消息发送到列表中, 在线程通过消费这个列表, 当消息满足batch_size个字符的数据的发送条件时, 会讲batch_size个字符的数据用socket发送给kafka, 发送完成会将连接放入_sending. 这个线程是一个循环调用run_once方法, 在run_once发送中调用发送逻辑, 发送完成后讲_sending中conn注册到kqueue中, 等待连接回调. 因为KafkaProducer使用的是kqueue, 所以不会碰到和SimpleProducer一样的问题. 后线程的实现类是Sender, kafka连接类是BrokerConnection, kafka client类是client_async. KafkaClient, selector类是vendor&#x2F;selectors34.py. KqueueSelector 问题:kafkaProducer当发送数据比较多时, 发送次数也会多, 当在django中请求比较少时, 发送到列表中数据也少, 发送一个请求发送到kafka中, 可能连续都完成了, 发送很快, 当请求变多时, 发送到列表中的数据变的无序, 不一定属于那个哪个请求的哪一次kafka发送, 因此会导致请求的数据发送到kafka变慢. 1234567from kafka import KafkaProducerhost = &#x27;10.8.100.7:9092&#x27;producer = KafkaProducer(bootstrap_servers=host)messages = [&#x27;messageA&#x27;,&#x27;messageB&#x27;]topic = &#x27;topic&#x27;for message in messages: producer.send(topic,messages) kafka 消费的坑kafka积压类问题https://juejin.cn/post/7314509615159885875kafka如何处理大量积压消息https://www.cnblogs.com/even160941/p/17010989.html 数据积压问题在大数据的场景下使用 MQ 一定会遇到数据积压的问题 Kafka消息积压的问题, 核心原因是生产太快、消费太慢, 处理速度长期失衡, 从而导致消息积压(Lag)的场景, 积压到超过队列长度限制, 就会出现还未被消费的数据产生丢失的场景. 生产者生产者异常导致重复生产数据, 需要考虑代码健壮性;服务器的ACK级别设置为-1, 即 At Least Once 可以保证数据不丢失, 但是不能保证数据不重复; 处理方式: 幂等性 Exactly Once，数据不会重复生产【0.11版本的kafka才会有的特性】 优化生产逻辑，做好监控预警功能 消费者消费者业务流程复杂, 消费速率远远小于生产速录, 产生消息积压; 涉及到 IO 的交互和网络的交互, 比如数据库SQL执行慢, 调用其他系统的API慢等总结有两方面原因: 消费能力不足: 消费的慢, 逻辑问题 数据处理&#x2F;拉取能力不足:(拉取数据&#x2F;处理时间 &lt; 生产速度) 第一种处理方式: 对partition扩容增加partition数量 将消费者代码修改为多线程并发消费 提高单条消息的处理速度, 如: 优化业务流程, 增加缓存, 去掉耗时操作 第二种处理方式: max.poll.records &#x3D; 20, 而 max.poll.interval.ms &#x3D; 1000, 也就是说consumer一次最多拉取 20 条消息, 两次拉取的最长时间间隔为 1 秒.也就是说消费者拉取的20条消息必须在1秒内处理完成, 紧接着拉取下一批消息. 否则, 超过1秒后, Kafka broker会认为该消费者处理太缓慢而将他踢出消费组, 从而导致消费组rebalance.根据Kafka机制, 消费组rebalance过程中是不会消费消息的, 所以看到ip是B和C轮流拉取消息, 又轮流被踢出消费组, 消费组循环进行rebalance, 消费就堆积了 处理方案: 消费者客户端减小 max.poll.records 或 增加 max.poll.interval.ms . RD 将 max.poll.records 设置为 1, 重启消费者后消费恢复 重复消费问题","categories":[{"name":"大数据","slug":"大数据","permalink":"https://champion-yang.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://champion-yang.github.io/tags/Kafka/"}]},{"title":"kafka架构理解和基本概念","slug":"02_kafka/2024-07-05-kafka01","date":"2024-07-05T08:26:02.000Z","updated":"2024-07-09T08:54:49.049Z","comments":true,"path":"2024/07/05/02_kafka/2024-07-05-kafka01/","permalink":"https://champion-yang.github.io/2024/07/05/02_kafka/2024-07-05-kafka01/","excerpt":"","text":"为什么要用消息队列 解耦允许你独立的扩展或修改两边的处理过程, 只要确保它们遵守同样的接口约束. 可恢复性系统的一部分组件失效时, 不会影响到整个系统. 消息队列降低了进程间的耦合度, 所以即使一个处理消息的进程挂掉, 加入队列中的消息仍然可以在系统恢复后被处理. 缓冲有助于控制和优化数据流经过系统的速度, 解决生产消息和消费消息的处理速度不一致的情况. 灵活性与峰值处理能力在访问量剧增的情况下, 应用仍然需要继续发挥作用, 但是这样的突发流量并不常见. 如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费. 使用消息队列能够使关键组件顶住突发的访问压力, 而不会因为突发的超负荷的请求而完全崩溃. 异步通信很多时候, 用户不想也不需要立即处理消息. 消息队列提供了异步处理机制, 允许用户把一个消息放入队列, 但并不立即处理它. 想向队列中放入多少消息就放多少, 然后在需要的时候再去处理它们. Kafka适合以下应用场景 日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer. 消息系统：解耦生产者和消费者、缓存消息等。 用户活动跟踪：kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动, 这些活动信息被各个服务器发布到kafka的topic中, 然后消费者通过订阅这些topic来做实时的监控分析, 亦可保存到数据库. 运营指标：kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈, 比如报警和报告; 流式处理：比如spark和flink。 基本概念 Producer ：消息生产者，就是向kafka broker发消息的客户端。 Consumer ：消息消费者，向kafka broker取消息的客户端。 Consumer Group （CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据, 一个分区只能由一个组内消费者消费; 消费者组之间互不影响. 所有的消费者都属于某个消费者组, 即消费者组是逻辑上的一个订阅者. Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic. Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic。 Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition, 每个partition是一个有序的队列. Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作, kafka提供了副本机制, 一个topic的每个分区都有若干个副本, 一个leader和若干个follower. leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader. follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时, 某个follower会成为新的follower. kafka为什么要分区 方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成, 因此整个集群就可以适应任意大小的数据了. 可以提高并发，因为可以以Partition为单位读写。 Kafka生产者分区策略 指明 partition 的情况下，直接将指明的值直接作为partiton值。 没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行取余得到partition值. 既没有partition值又没有key值的情况下，第一次调用时随机生成一个整数(后面每次调用在这个整数上自增), 将这个值与topic可用的partition总数取余得到partition值, 也就是常说的round-robin算法. kafka的数据可靠性怎么保证为保证producer发送的数据, 能可靠的发送到指定的topic, topic的每个partition收到producer发送的数据后, 都需要向producer发送ack(acknowledgement确认收到), 如果producer收到ack, 就会进行下一轮的发送, 否则重新发送数据. 所以引出ack机制. 数据重复和丢失问题Kafka为用户提供了三种可靠性级别, 用户根据对可靠性和延迟的要求进行权衡, 选择以下的配置. acks参数配置: 0:producer不等待broker的ack, 这一操作提供了一个最低的延迟, broker一接收到还没有写入磁盘就已经返回, 当broker故障时有可能丢失数据. 1:producer等待broker的ack, partition的leader落盘成功后返回ack, 如果在follower同步成功之前leader故障, 那么将会丢失数据. -1(all):producer等待broker的ack, partition的leader和follower全部落盘成功后才返回ack.但是如果在follower同步完成后, broker发送ack之前, leader发生故障, 那么会造成数据重复. Kafka消费能力不足怎么处理 如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数&#x3D;分区数.(两者缺一不可) 如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据&#x2F;处理时间&lt;生产速度), 使处理的数据小于生产的数据, 也会造成数据积压. Kafka中的数据是有序的吗单分区内有序.多分区, 分区与分区间无序. Kafka可以按照时间消费数据吗可以, 提供的API方法:KafkaUtil.fetchOffsetsWithTimestamp(topic, sTime, kafkaProp) Kafka单条日志传输大小kafka对于消息体的大小默认为单条最大值是1M但是在我们应用场景中, 常常会出现一条消息大于1M,如果不对kafka进行配置. 则会出现生产者无法将消息推送到kafka或消费者无法去消费kafka里面的数据, 这时我们就要对kafka进行以下配置:server.properties 123replica.fetch.max.bytes: 1048576 # broker可复制的消息的最大字节数, 默认为1Mmessage.max.bytes: 1000012 # kafka 会接收单个消息size的最大限制， 默认为1M左右# 注意 message.max.bytes必须小于等于replica.fetch.max.bytes，否则就会导致replica之间数据同步失败 引用 kafka官方中文文档 公众号: 大数据左右手 超详细的Kafka教程-从部署到开发到原理都有讲解","categories":[{"name":"大数据","slug":"大数据","permalink":"https://champion-yang.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://champion-yang.github.io/tags/Kafka/"}]},{"title":"2024-07-04-pandas03","slug":"01_pandas/2024-07-04-pandas03","date":"2024-07-04T07:33:47.000Z","updated":"2024-07-04T08:41:53.015Z","comments":true,"path":"2024/07/04/01_pandas/2024-07-04-pandas03/","permalink":"https://champion-yang.github.io/2024/07/04/01_pandas/2024-07-04-pandas03/","excerpt":"","text":"","categories":[{"name":"数据科学","slug":"数据科学","permalink":"https://champion-yang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"},{"name":"数据处理","slug":"数据科学/数据处理","permalink":"https://champion-yang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}],"tags":[]},{"title":"pandas 处理 excel 表格合并操作","slug":"01_pandas/2024-07-04-pandas02","date":"2024-07-04T07:29:31.000Z","updated":"2024-07-23T06:32:44.542Z","comments":true,"path":"2024/07/04/01_pandas/2024-07-04-pandas02/","permalink":"https://champion-yang.github.io/2024/07/04/01_pandas/2024-07-04-pandas02/","excerpt":"","text":"本文主要记录利用 pandas 操作 excel 中, 表格合并操作的代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187import pandas as pdfrom openpyxl import load_workbookimport timeimport openpyxlfrom openpyxl.styles import *import osfrom datetime import date&quot;&quot;&quot;解析大表，输出 excel, 每一列生成一份 excel 文件&quot;&quot;&quot;# 定义合并单元格的函数def Merge_cells(ws, target_list, start_row, col, end_row=None): &#x27;&#x27;&#x27; ws: 是需要操作的工作表 target_list: 是目标列表，即含有重复数据的列表 start_row: 是开始行，即工作表中开始比对数据的行（需要将标题除开） col: 是需要处理数据的列 &#x27;&#x27;&#x27; start = 0 # 开始行计数，初试值为0，对应列表中的第1个元素的位置0 end = 0 # 结束行计数，初试值为0，对应列表中的第1个元素的位置0 reference = target_list[0] # 设定基准，以列表中的第一个字符串开始 if end_row: target_list = target_list[start_row - 1:end_row] reference = target_list[0] # 设定基准，以列表中的第一个字符串开始 # print(&quot;target_list&quot;, target_list) # print(&quot;reference&quot;, reference) for i in range(len(target_list)): # 遍历列表 if target_list[i] != reference: # 开始比对，如果内容不同执行如下 reference = target_list[i] # 基准变成列表中下一个字符串 end = i - 1 # 列计数器 ws.merge_cells(col + str(start + start_row) + &quot;:&quot; + col + str(end + start_row)) start = end + 1 if i == len(target_list) - 1: # 遍历到最后一行，按如下操作 end = i ws.merge_cells(col + str(start + start_row) + &quot;:&quot; + col + str(end + start_row)) reference = None else: for i in range(len(target_list)): # 遍历列表 if target_list[i] != reference: # 开始比对，如果内容不同执行如下 reference = target_list[i] # 基准变成列表中下一个字符串 end = i - 1 # 列计数器 ws.merge_cells(col + str(start + start_row) + &quot;:&quot; + col + str(end + start_row)) start = end + 1 if i == len(target_list) - 1: # 遍历到最后一行，按如下操作 end = i ws.merge_cells(col + str(start + start_row) + &quot;:&quot; + col + str(end + start_row))# 获取Excel表格中的数据# 设置边框&#123;&#x27;medium&#x27; 中粗 &#x27;thin&#x27; 细 &#x27;thick&#x27; 粗 &#x27;dashed&#x27; 虚线 &#x27;dotted&#x27; 点线&#125;def format_border_cell(ws, row_index, col_index): ws.cell(row_index, col_index).border = Border(top=Side(border_style=&#x27;thin&#x27;, color=&#x27;FF000000&#x27;), right=Side(border_style=&#x27;thin&#x27;, color=&#x27;FF000000&#x27;), bottom=Side(border_style=&#x27;thin&#x27;, color=&#x27;FF000000&#x27;), left=Side(border_style=&#x27;thin&#x27;, color=&#x27;FF000000&#x27;))def set_schema(file_path): wb = load_workbook(file_path) sheet_names = wb.get_sheet_names() for sheet_name in sheet_names: # 遍历每个工作表，抓取数据，并根据要求合并单元格 ws = wb[sheet_name] target_list = [] # 考核类型 customer_list = [] _lst_2 = [] _lst_3 = [] _lst_4 = [] _lst_5 = [] _lst_6 = [] for row in range(1, 66): customer = ws[&#x27;A&#x27; + str(row)].value customer_list.append(customer) _2 = ws[&#x27;B&#x27; + str(row)].value _lst_2.append(_2) _3 = ws[&#x27;C&#x27; + str(row)].value _lst_3.append(_3) _4 = ws[&#x27;D&#x27; + str(row)].value _lst_4.append(_4) _5 = ws[&#x27;E&#x27; + str(row)].value _lst_5.append(_5) _6 = ws[&#x27;F&#x27; + str(row)].value _lst_6.append(_6) start_row = 1 # 开始行是第六行 Merge_cells(ws, customer_list, start_row, &quot;A&quot;) Merge_cells(ws, _lst_2, start_row, &quot;B&quot;) Merge_cells(ws, _lst_3, start_row, &quot;C&quot;) # Merge_cells(ws, _lst_4, start_row, &quot;D&quot;) Merge_cells(ws, _lst_4, 2, &quot;D&quot;, end_row=13) Merge_cells(ws, _lst_4, 14, &quot;D&quot;, end_row=24) Merge_cells(ws, _lst_4, 25, &quot;D&quot;, end_row=27) Merge_cells(ws, _lst_4, 28, &quot;D&quot;, end_row=32) # Merge_cells(ws, _lst_4, 28, &quot;D&quot;, end_row=33) Merge_cells(ws, _lst_4, 33, &quot;D&quot;, end_row=34) Merge_cells(ws, _lst_4, 35, &quot;D&quot;, end_row=38) Merge_cells(ws, _lst_4, 39, &quot;D&quot;, end_row=41) Merge_cells(ws, _lst_4, 42, &quot;D&quot;, end_row=49) Merge_cells(ws, _lst_4, 50, &quot;D&quot;, end_row=56) Merge_cells(ws, _lst_4, 57, &quot;D&quot;, end_row=65) Merge_cells(ws, _lst_5, start_row, &quot;E&quot;) Merge_cells(ws, _lst_6, start_row, &quot;F&quot;) alignment_center = Alignment(horizontal=&#x27;left&#x27;, vertical=&#x27;center&#x27;, wrapText=True) # 创建边框线对象 border = Border(top=Side(border_style=&#x27;thick&#x27;, color=&#x27;FF000000&#x27;), right=Side(border_style=&#x27;thick&#x27;, color=&#x27;FF000000&#x27;), bottom=Side(border_style=&#x27;thick&#x27;, color=&#x27;FF000000&#x27;), left=Side(border_style=&#x27;thick&#x27;, color=&#x27;FF000000&#x27;)) # 指定区域单元格居中 ws_area = ws[&quot;A1:H65&quot;] for i in ws_area: for j in i: j.alignment = alignment_center # j.border = border font1 = Font(name=&quot;黑体&quot;) font2 = Font(name=&quot;黑体&quot;, bold=True) # 指定区域单元格字体 ws_area = ws[&quot;A1:H1&quot;] k = 0 for i in ws_area: k = k + 1 for j in i: if (k == 1): j.font = font2 else: j.font = font1 ws.column_dimensions[&#x27;A&#x27;].width = 20.0 ws.column_dimensions[&#x27;B&#x27;].width = 20.0 ws.column_dimensions[&#x27;C&#x27;].width = 20.0 ws.column_dimensions[&#x27;D&#x27;].width = 20.0 ws.column_dimensions[&#x27;E&#x27;].width = 30.0 ws.column_dimensions[&#x27;F&#x27;].width = 30.0 ws.column_dimensions[&#x27;G&#x27;].width = 40.0 ws.column_dimensions[&#x27;H&#x27;].width = 30.0 wb.save(file_path)def main(p1): # 读取 Excel 文件 file_path = p1 df = pd.read_excel(file_path, engine=&#x27;openpyxl&#x27;, sheet_name=&quot;xxx&quot;) # 获取所有列名 column_names_0_5 = df.columns[:7] column_names_6_ = df.columns[7:] # 写入 excel for column_name in column_names_6_: # 提取当前列的数据 column_name_lst = list(column_names_0_5) + [str(column_name)] column_data = df[column_name_lst] pd_column_data = column_data.ffill() column_data_one = set(df[str(column_name)].values) # 在当前路径下创建文件夹 folder_name_lst = [&quot;&#123;&#125;\\xxx路径\\&#123;&#125;&quot;.format(str(date.today()), i) for i in [&quot;移动&quot;,&quot;联通&quot;,&quot;电信&quot;]] for folder_name in folder_name_lst: if not os.path.exists(folder_name): os.makedirs(folder_name) if &quot;移动&quot; in column_name: new_file_name = &quot;&#123;&#125;\\xxx路径\\&#123;&#125;\\\\&quot;.format(str(date.today()), &quot;移动&quot;) + column_name + &quot;.xlsx&quot; elif &quot;联通&quot; in column_name: new_file_name = &quot;&#123;&#125;\\xxx路径\\&#123;&#125;\\\\&quot;.format(str(date.today()), &quot;联通&quot;) + column_name + &quot;.xlsx&quot; elif &quot;电信&quot; in column_name: new_file_name = &quot;&#123;&#125;\\xxx路径\\&#123;&#125;\\\\&quot;.format(str(date.today()), &quot;电信&quot;) + column_name + &quot;.xlsx&quot; else: raise &quot;=================&quot; # 创建新的 sheet，并将列名作为 sheet 名称 new_sheet_name = f&#x27;&#123;column_name&#125;&#x27; pd_column_data.to_excel(new_file_name, sheet_name=new_sheet_name, index=False) set_schema(new_file_name)if __name__ == &quot;__main__&quot;: file_path = r&#x27;./文件.xlsx&quot; main(file_path)","categories":[{"name":"数据科学","slug":"数据科学","permalink":"https://champion-yang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"},{"name":"数据处理","slug":"数据科学/数据处理","permalink":"https://champion-yang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"https://champion-yang.github.io/tags/Pandas/"}]},{"title":"研发工程视角下的数据分析(数据处理)","slug":"01_pandas/2024-07-04-pandas01","date":"2024-07-04T06:06:55.000Z","updated":"2024-07-08T03:02:55.991Z","comments":true,"path":"2024/07/04/01_pandas/2024-07-04-pandas01/","permalink":"https://champion-yang.github.io/2024/07/04/01_pandas/2024-07-04-pandas01/","excerpt":"","text":"背景在大数据时代的背景下, 数据已经成为企业中最重要的资产和生产要素. 如何从数据中发现并挖掘有价值的信息则显得尤为重要. 在项目交付过程中, 数据是平台&#x2F;xx系统的基础, 而作为研发人员经常会面对客户的提数需求, 这部分需求经常具备个性化、临时性、一次性等特点. 如下作为示例: 产品需求: 系统上线后数据异常原因排查; 新老版本数据融合需求; 数据报表需求等 运营需求: 某次任务关联的报告数据; 通过系统数据做趋势预测; 数据变动的原因分析等 客户需求: 提取某次数据上报过程中某省单位的上报详情并出具分析报告; 查一下某个时间点下系统数据的合规率等本系列的文章介绍的数据分析区别于BI工程师【数据分析师】的数据分析, 现实情况中BI们往往在做业务方最关心的数据、最关心的问题等方向的分析, 对于日常过程中的个性化、临时性的数分需求, 一般都是通过研发工程师进行实现. 本系列文章的重点将会以研发工程师【非BI】的视角出发, 介绍如何通过 Python 快速进行数据分析. 数据分析的流程知其然, 需知其所以然. 数据科学是一门复杂的学科, 包括了统计学、数据分析、机器学习在内的多种学科方法, 而数据分析则关注在现有的数据集中, 执行和处理统计分析. 体系化的了解数据分析的流程, 是做数据分析的前置条件.常规的数据分析流程如下: 明确分析思路和目的: 我们接到一个分析任务, 首先要弄清楚我们分析的对象是什么, 要达成怎样的目的, 不能陷于为了分析而分析. 然后, 要熟悉行业和业务, 透彻的理解分析的目的, 构建起分析的角度和体系. 数据收集: 按照确定的数据分析的思路和框架, 进行数据收集和整合. 数据处理: 对收集到的数据进行清洗、加工、整理等 数据分析: 通过分析手段、方法和技巧等对准备好的数据进行探索分析, 从中发现数据的规律. 数据展现: 通过图表或者报告的形式进行数据到的展现. 从研发视角看数据分析【数据处理篇】作为研发工程师, 在常规的项目中, 假定我们现在接收到的临时性的提数需求已经和产品经理&#x2F;项目经理就数分的思路和目的达成了一致, 并且系统中的数据已经通过南向接口或其他方式进行了收集, 下一步要展开数据处理过程.数据清洗是数据预处理的第一步, 主要是为了解决数据中的缺失值、异常值、重复值等问题. Python提供了pandas库可以帮助我们方便地处理源数据中的缺失值、重复值和异常值, 数据处理可以极大的提升数据的质量, 只有经过处理的数据才可以作为下一步数据分析模型的输入.pandas常用的基本功能如下: 从Excel、CSV、网页、SQL、剪贴板等文件或工具中读取数据; 合并多个文件或者电子表格中的数据, 将数据拆分为独立文件; 数据清洗, 如去重、处理缺失值、填充默认值、补全格式、处理极端值等; 建立高效的索引; 支持大体量数据; 按一定业务逻辑插入计算后的列、删除列; 灵活方便的数据查询、筛选; 分组聚合数据, 可独立指定分组后的各字段计算方式; 数据的转置, 如行转列、列转行变更处理; 连接数据库, 直接用SQL查询数据并进行处理; 对时序数据进行分组采样, 如按季、按月、按工作小时, 也可以自定义周期, 如工作日; 窗口计算, 移动窗口统计、日期移动等; 灵活的可视化图表输出, 支持所有的统计图形; 为数据表格增加展示样式, 提高数据识别效率. 实践应用准备工作环境准备1pip install pandas 数据集分析在准备员工一到四月的考核结果表, 各列说明如下: name: 员工的姓名, 这列没有重复值, 一个员工一行 team: 所在的团队, 这个数据会重复 一月～四月: 各个月份的考核结果, 可能会有重复值, 缺失值, 异常值 使用示例缺失值处理检查是否有缺失值关键技术: isnull()方法返回值为布尔值, 如果数据存在缺失值, 返回True; 否则, 返回False. 经查, 原始数据中具备缺失值. 需求案例A: 如果该行具有缺失值, 则这一行数据被丢弃关键技术: dropna()用于删除含有缺失值的行. 需求案例B: 如果该行具有缺失值, 则缺失的值使用平均值进行填充关键技术: fillna()方法用于填充含有缺失值的行.mean()方法用于求改行的平均值. 重复值处理检查是否有重复值关键技术: duplicated()方法检测冗余的行或列, 默认是判断全部列中的值是否全部重复, 并返回布尔类型的结果. 对于完全没有重复的行, 返回值为False. 对于有重复值的行, 第一次出现重复的那一行返回False, 其余的返回True. 需求案例C: 如果该行有重复值则丢弃该行数据关键技术: drop_duplicates()删除重复的行. 异常值检测和处理检查是否有异常值关键技术:query()方法查询数据中是否有异常值, 用于在 DataFrame 中执行类似于 SQL 的查询的工具, 支持简单查询, 逻辑运算查询, 字符串查询等. 输出新的文件需求案例D: 根据描述输出报告需求描述: 将原始数据根据姓名进行分组, 缺失值和异常值默认按照0进行统计, 计算每个人的平均分并从大到小进行排序, 输出内容到 excel.代码: 12345678910111213141516171819202122232425262728293031# 引入Pandas库import pandas as pd# 读取excel内容df = pd.read_excel(&quot;./数据集/team.xlsx&quot;, )# 用 0 填充缺失值df = df.fillna(0)# 删除重复的行df = df.drop_duplicates().copy()# 缺失值和异常值默认按照0进行填充df[[&quot;一月&quot;, &quot;二月&quot;, &quot;三月&quot;, &quot;四月&quot;]] = df[[&quot;一月&quot;, &quot;二月&quot;, &quot;三月&quot;, &quot;四月&quot;]].applymap(lambda x: 0 if x &gt; 100 or x &lt; 0 else x)# 增加一列, 平均值df[&#x27;平均值&#x27;] = df[[&quot;一月&quot;, &quot;二月&quot;, &quot;三月&quot;, &quot;四月&quot;]].mean(axis=1).round(1)# 按照平均值排序df = df.sort_values([&#x27;平均值&#x27;], ascending=False)# 将数据输出到Excel文件中df.to_excel(&#x27;./output.xlsx&#x27;, index=False) 输出的excel结果如下: 总结本文简单介绍了数据分析的流程以及如何做数据的预处理. 利用几个示例对工作中常见的操作进行了说明, 从代码的层面可以看出, 利用 Python 进行数据分析的操作会更加的灵活快捷方便, 尤其是在临时性的需求面前, 会大大节约我们的时间和资源成本.","categories":[{"name":"数据科学","slug":"数据科学","permalink":"https://champion-yang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"},{"name":"数据处理","slug":"数据科学/数据处理","permalink":"https://champion-yang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"https://champion-yang.github.io/tags/Pandas/"}]},{"title":"备考系统架构师","slug":"备考系统架构师","date":"2024-07-04T01:31:28.000Z","updated":"2024-07-04T08:43:13.335Z","comments":true,"path":"2024/07/04/备考系统架构师/","permalink":"https://champion-yang.github.io/2024/07/04/%E5%A4%87%E8%80%83%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/","excerpt":"","text":"","categories":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://champion-yang.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"}],"tags":[{"name":"架构","slug":"架构","permalink":"https://champion-yang.github.io/tags/%E6%9E%B6%E6%9E%84/"}]},{"title":"明朝那些事","slug":"明朝那些事","date":"2024-07-04T01:31:28.000Z","updated":"2024-07-04T08:43:26.683Z","comments":true,"path":"2024/07/04/明朝那些事/","permalink":"https://champion-yang.github.io/2024/07/04/%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B/","excerpt":"","text":"这是测试信息","categories":[{"name":"读书","slug":"读书","permalink":"https://champion-yang.github.io/categories/%E8%AF%BB%E4%B9%A6/"}],"tags":[{"name":"书籍","slug":"书籍","permalink":"https://champion-yang.github.io/tags/%E4%B9%A6%E7%B1%8D/"}]},{"title":"探究Docker镜像","slug":"探究Docker镜像","date":"2024-07-02T07:32:45.000Z","updated":"2024-07-04T08:43:18.695Z","comments":true,"path":"2024/07/02/探究Docker镜像/","permalink":"https://champion-yang.github.io/2024/07/02/%E6%8E%A2%E7%A9%B6Docker%E9%95%9C%E5%83%8F/","excerpt":"","text":"Docker镜像简单介绍 到底什么是镜像? 镜像是用来干嘛的? 镜像的构成是怎样的? Docker镜像使用 获取镜像 查看镜像 运行镜像 删除镜像 定制镜像 构建镜像的其他方式 Docker镜像简单介绍要知道Docker镜像是怎么一回事, 得先知道Docker是怎么一回事:Docker 是一个开源的、轻量级的容器引擎, 主要运行于 Linux 和 Windows, 用于创建、管理和编排容器. 为了让应用程序既可以运行在统一主机或者集群, 又能彼此隔离, 虚拟机出现了. 但是虚拟机需要自己的操作系统, 一般体积比较庞大不好维护和升级, 所以容器出现了. 容器目的就是运行应用或者服务, 这意味着容器的镜像中必须包含应用&#x2F;服务运行所必需的操作系统和应用文件. ok, 进入主题! 到底什么是镜像?我们都知道操作系统分为内核和用户空间, 对于 Linux 而言, 内核启动后会挂在 root 文件系统为其提供用户空间的支持. 而 Docker 镜像 就相当于一个 root 文件系统, 提供容器运行时所需的程序、库、资源、配置等文件以及运行时的参数(环境变量、用户、数据卷等). 镜像是用来干嘛的?docker 镜像代表了容器的文件系统里的内容, 是容器的基础, 镜像一般是通过 Dockerfile 生成的, 容器是镜像运行时的实体. 镜像的构成是怎样的?在 Docker 的设计上采用了 Union FS 的技术, 分层存储的架构. 所以严格意义上来讲镜像是一个虚拟的概念, 其体现并非由一个文件组成, 而是由一组文件系统组成, 或者说是由多层文件系统联合组成.简单理解, docker 的镜像是分层的, 所有的镜像(除了基础镜像)都是在之前镜像的基础上加上自己这层的内容生成的. Docker镜像使用获取镜像镜像构建完成后, 我们可以将其上传到仓库中, 供由其他服务器使用. 提供集中存储和分发镜像的服务是 Docker Registry, 官方发布的公开服务地址是 https://hub.docker.com/, 国内也有云服务商发布类似公开的服务, 比如阿里云 https://cn.aliyun.com/product/acr?source=5176.11533457&amp;userCode=8lx5zmtu除了使用公开服务外, 我们还可以在本地搭建私有的 Docker Registry. 通过命令 docker pull 从仓库获取镜像, 不指定 Docker Registry 默认从 docker hub 拉取镜像. 如图我们以拉取 mysql5.6 镜像为例: docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] 查看镜像使用 docker image ls 命令查看所有已经下载的镜像, 如下图所示可以看到我们下载的 mysql5.6 镜像. docker image ls 通过 docker image ls mysql 命令可以查看指定名称的镜像 docker image ls mysql 运行镜像 docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:5.6 查看运行中的容器: docker ps 进入容器内部: docker exec -it b22754ecb0a9 /bin/bash 删除镜像 docker image rm [选项] &lt;镜像1&gt; [&lt;镜像2&gt; ...] 其中, &lt;镜像&gt; 可以是 镜像短 ID、镜像长 ID、镜像名 或者 镜像摘要等.像其它可以承接多个实体的命令一样, 可以使用 docker image ls -q 来配合使用 docker image rm, 这样可以成批的删除希望删除的镜像. docker image rm $(docker image ls -q mysql) 定制镜像镜像构建时, 会一层层构建, 前一层是后一层的基础. 每一层构建完就不会再发生改变, 后一层上的任何改变只发生在自己这一层. 比如, 删除前一层文件的操作, 实际不是真的删除前一层的文件, 而是仅在当前层标记为该文件已删除. 在最终容器运行的时候, 虽然不会看到这个文件, 但是实际上该文件会一直跟随镜像. 因此, 在构建镜像的时候, 需要额外小心, 每一层尽量只包含该层需要添加的东西, 任何额外的东西应该在该层构建结束前清理掉.镜像的定制实际上就是定制每一层所添加的配置、文件. 如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本, 用这个脚本来构建、定制镜像, 那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决. 这个脚本就是 Dockerfile. 以定制 nginx 镜像为例: 在一个空白目录中，建立一个文本文件，并命名为 Dockerfile Dockerfile 内写入以下内容 FROM nginx RUN echo &#39;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#39; &gt; /usr/share/nginx/html/index.html 构建 nginx 镜像 docker build -t nginx:v3 . 在 9529 端口运行 nginx 镜像 docker run --name webserver -d -p 9529:80 nginx:v3 开放 9529 端口可以被外部访问 iptables -I INPUT -p tcp --dport 9529 -j ACCEPT; 浏览器访问该地址，得到返回 “Hello, Docker!” 在示例中, 我们编写的 Dockerfile 文件很简单, 只有两条指令:: FROM nginx ——&gt; 指定基础镜像, 定制镜像一定是以一个镜像为基础, 在其上进行定制. 在 Dockerfile 中 FROM 是必备的指令. RUN xxx ——&gt; RUN 指令用来执行命令行命令. 从对下图的分析诠释在 Dockerfile 中, 每一个指令都会建立一层的概念:在 step1 中 FROM 指定拉取仓库中的 nginx 并提交了最新的一层 605c77e624dd.在 step2 中 RUN 指令 开启了容器 d25b52a4b879 , 执行了 echo 的命令将银行中的内容覆盖输出到 nginx 目录下的 index.html 文件中, 然后删除了该容器 d25b52a4b879, 并最后提交了一层 b3db9b911471 . 构建镜像的其他方式除却使用 Dockerfile 生成镜像的方式, 我们还可以通过其他方式构建: 从 rootfs 压缩包导入。 docker import [选项] &lt;文件&gt;|&lt;URL&gt;|- [&lt;仓库名&gt;[:&lt;标签&gt;]] 压缩包可以是本地文件、远程 Web 文件, 甚至是从标准输入中得到. 压缩包将会在镜像目录展开, 并直接作为镜像第一层提交. 将构建好的 docker 镜像直接导入导出 docker 提供了 docker save 和 docker load 命令, 用以将镜像保存为一个文件, 然后传输到另一个位置上, 再进行加载. docker save nginx:v3 | gzip &gt; nginx-v3.tar.gz","categories":[{"name":"技术","slug":"技术","permalink":"https://champion-yang.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://champion-yang.github.io/tags/Docker/"}]},{"title":"Mac 通过 brew 安装 redis, 搭建 redis 集群","slug":"Mac 通过 brew 安装 redis，搭建 redis 集群","date":"2024-07-01T10:05:47.000Z","updated":"2024-07-04T08:37:38.486Z","comments":true,"path":"2024/07/01/Mac 通过 brew 安装 redis，搭建 redis 集群/","permalink":"https://champion-yang.github.io/2024/07/01/Mac%20%E9%80%9A%E8%BF%87%20brew%20%E5%AE%89%E8%A3%85%20redis%EF%BC%8C%E6%90%AD%E5%BB%BA%20redis%20%E9%9B%86%E7%BE%A4/","excerpt":"","text":"一、对集群的初步了解 二、为什么要搭建集群 三、如何搭建 redis 集群 1.redis 安装, 查看redis安装路径 2. 集群环境配置 四、测试 五、知识点 六、参考 本篇通过灵魂三问来探究 redis 集群: 是什么? 为什么? 怎么做? 一、对集群的初步了解todo… 二、为什么要搭建集群todo… 三、如何搭建 redis 集群1.redis 安装, 查看redis安装路径 安装 1brew install redis redis.conf 路径:&#x2F;opt&#x2F;homebrew&#x2F;etc&#x2F;redis.conf ps: 这里路径及查找比较麻烦, 1vim /opt/homebrew/Cellar/redis/7.0.4/homebrew.mxcl.redis.plist 2. 集群环境配置 mkdir 以下目录, 1cp /opt/homebrew/etc/redis.conf /usr/local/etc/redis/cluster/7000/7000.conf 1234567-------------- 修改 7000.conf ，其他文件类似 ----------port 7000 # Redis 节点的端口号cluster-enabled yes # 实例以集群模式运行cluster-config-file nodes-7000.conf # 节点配置文件路径cluster-node-timeout 5000 # 节点间通信的超时时间appendonly yes # 数据持久化----------------------------------------- 集群目录结构 123456789101112131415/usr/local/etc`-- redis `-- cluster |-- 7000 | `-- 7000.conf |-- 7001 | `-- 7001.conf |-- 7002 | `-- 7002.conf |-- 7003 | `-- 7003.conf |-- 7004 | `-- 7004.conf `-- 7005 `-- 7005.conf 启动 redis 的六个节点: 123redis-server /usr/local/etc/redis/cluster/7000/7000.conf &amp;redis-server /usr/local/etc/redis/cluster/7001/7001.conf &amp;其他依次顺延 查看启动状态: 查看版本号:1、启动服务端:redis-server2、启动客户端:redis-cli3、客户端输入:info 创建集群(redis 5 和redis 7 创建集群的命令不一致, 示例是 redis 7) 1redis-cli --cluster create --cluster-replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 warning: 不要将主从数据库放在同一台机器上, 避免宕机数据丢失M:master 主节点S:slave 从节点三主三从7000\\7001\\7002 主节点7003、7004、7005 从节点 四、测试 集群搭建成功 下线一个主节点: 查看节点状态: 这个时间 7002 节点是 fail 状态, 并且 7004 成为了新的 master 节点 下线一个从节点 7005 上线一个从节点 7005 1redis-server /usr/local/etc/redis/cluster/7005/7005.conf &amp; 上线一个主节点 7002这个时候已经有三个主节点了 这个时候 7002 上线成功, 不过成为了 从节点 五、知识点集群节点状态说明 主节点 主节点存在hash slots, 且主节点的hash slots 没有交叉主节点不能删除 一个主节点可以有多个从节点 主节点宕机时多个副本之间自动选举主节点 从节点 从节点没有hash slots 从节点可以删除 六、参考https://segmentfault.com/a/1190000022808576「必看」 http://redisdoc.com/topic/cluster-tutorial.html#id2「必看」 https://blog.csdn.net/qq_43439968/article/details/109882660 https://zhuanlan.zhihu.com/p/59172042 https://blog.csdn.net/qq_29329241/article/details/113326613","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://champion-yang.github.io/tags/Redis/"}]},{"title":"redis缓存一致性","slug":"redis缓存一致性","date":"2024-07-01T09:55:31.000Z","updated":"2024-07-04T08:43:08.179Z","comments":true,"path":"2024/07/01/redis缓存一致性/","permalink":"https://champion-yang.github.io/2024/07/01/redis%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/","excerpt":"","text":"redis 缓存一致性 一、缓存的设计 二、数据一致性问题的出现 三、缓存一致性的解决思路 1. 常用方法: 先更新 db, 后删除 cache 2. 高并发的时候, 后删缓存还是存在缓存不一致的问题(归根结底还是”删除操作”发生在”更新操作”之前) 四、缓存击穿问题 redis 缓存一致性在项目中发现大家使用 redis 缓存, 不太注重一致性的设计, 所以整理本文, 希望能给大家一些帮助 一、缓存的设计首先, 读缓存;如果缓存里没有值, 那就读取数据库的值;同时把这个值写进缓存中. 二、数据一致性问题的出现双更新模式: 操作不合理, 导致数据一致性问题 比如我要更新一个值, 首先刷了缓存, 然后把数据库也更新了. 但过程中, 更新数据库可能会失败, 发生了回滚. 所以, 最后”缓存里的数据”和”数据库的数据”就不一样了, 也就是出现了数据一致性问题. 你或许会说: 我先更新数据库, 再更新缓存不就行了? 这依然会有问题. 考虑到下面的场景: 操作 A 更新 a 的值为 1, 操作 B 更新 a 的值为 2. 由于数据库和 Redis 的操作, 并不是原子的, 它们的执行时长也不是可控制的. 当两个请求的时序发生了错乱, 就会发生缓存不一致的情况. 三、缓存一致性的解决思路1. 常用方法: 先更新 db, 后删除 cache数据的读取过程, 规则是”先读 cache, 再读 db”, 详细步骤如下: 每次读取数据, 都从 cache 里读; 如果读到了, 则直接返回, 称作 cache hit; 如果读不到 cache 的数据, 则从 db 里面捞一份, 称作 cache miss; 将读取到的数据塞入到缓存中, 下次读取时, 就可以直接命中. 写请求, 规则是”先更新 db, 再删除缓存”, 详细步骤如下: 将变更写入到数据库中; 删除缓存里对应的数据. 2. 高并发的时候, 后删缓存还是存在缓存不一致的问题(归根结底还是”删除操作”发生在”更新操作”之前)解决方案: 延时双删: 而假如我有一种机制, 能够确保删除动作一定被执行, 那就可以解决问题, 起码能缩小数据不一致的时间窗口. 常用的方法就是延时双删, 依然是先更新再删除, 唯一不同的是: 我们把这个删除动作, 在不久之后再执行一次, 比如 5 秒之后. 这种方案需要经过技术选型, 跟着项目走, 可以利用 mq, 延时队列等. 闪电缓存: 把缓存的失效时间设置非常短, 比如 3～4 秒. 一旦失效, 就会再次去数据库读取最新数据到缓存. 但这种方式, 在非常高的并发下, 同一时间对某个 key 的请求击穿到 DB, 会锁死数据库, 所以很少用. 四、缓存击穿问题缓存击穿, 指的是缓存中没有数据但数据库中有, 由于同一时刻请求量特别大, 但是没有读到缓存数据, 就会一股脑涌入到数据库中读取, 造成数据库假死. 任何删除缓存的动作都会造成缓存击穿. 所以我们上面一直说的是要删除缓存, 但在极高并发下, 你还不能乱删.那么该问题如何解决呢? 读操作互斥 集中更新 弱化数据库","categories":[{"name":"技术","slug":"技术","permalink":"https://champion-yang.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://champion-yang.github.io/tags/Redis/"}]},{"title":"DRF源码解析","slug":"DRF源码解析","date":"2024-07-01T01:35:59.000Z","updated":"2024-07-04T08:41:04.918Z","comments":true,"path":"2024/07/01/DRF源码解析/","permalink":"https://champion-yang.github.io/2024/07/01/DRF%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"DRF组件 1. Web应用模式 2. api接口 3. RESTful API规范 幂等性 4. Django的CBV 4.1 CBV的简单使用 4.2 CBV的源码解析 4.2.1 面向对象知识点回顾 (1)继承 (2)反射 4.2.2 CBV的源码解析 5. Django_Rest_Framework 6. 环境安装与配置 7. DRF的APIView视图 7.1 APIView的简单使用 7.2 APIView的源码解析 7.1、请求 7.1.1、常用属性 1).data 2).query_params 3)request._request 7.1.2、基本使用 7.2、响应 7.2.1 构造方式 7.2.2 response对象的属性 7.2.3 状态码 8. 序列化器-Serializer 8.1 定义序列化器 8.2 创建Serializer对象 8.3 序列化器的使用 8.3.1 序列化 (1)基本序列化 8.3.2 反序列化 (1)数据验证 (1) validate_字段名 (2) validate (2)反序列化-保存数据 (3) 附加说明 8.3.3、基于APIView的接口实现 8.4 模型类序列化器 (1)定义 (2)指定字段 (3)添加额外参数 9. 视图 9.1、GenericAPIView[通用视图类] (1)get_serializer_class(self) (2)get_serializer(self, *args, **kwargs) (3)get_queryset(self) (4)get_object(self) 9.2、5个视图扩展类 (1)ListModelMixin （2）CreateModelMixin （3）RetrieveModelMixin （4）UpdateModelMixin （5）DestroyModelMixin 9.3、 GenericAPIView的视图子类 （1）CreateAPIView （2）ListAPIView （3）RetrieveAPIView （4）DestoryAPIView （5）UpdateAPIView （6）ListCreateAPIView （7）RetrieveUpdateAPIView （8）RetrieveDestoryAPIView （9）RetrieveUpdateDestoryAPIView 9.4、视图集 9.4.1、ViewSet 9.4.2、GenericViewSet 9.4.3、ModelViewSet和ReadOnlyModelViewSet 10. 路由Routers 10.1 使用方法 10.2 视图集中附加action的声明 10.3 路由router形成URL的方式 11、其它功能组件 11.1. 认证Authentication 11.2. 权限Permissions 使用 提供的权限 举例 自定义权限 11.3. 限流Throttling 基本使用 可选限流类 11.4. 过滤Filtering 11.5. 排序Ordering 11.6. 分页Pagination 11.7. 异常处理 Exceptions REST framework定义的异常 11.8. 自动生成接口文档 11.8.1. 安装依赖 11.8.2. 设置接口文档访问路径 11.8.3. 文档描述说明的定义位置 11.8.4. 访问接口文档网页 DRF组件1. Web应用模式在开发Web应用中, 有两种应用模式: 前后端不分离[客户端看到的内容和所有界面效果都是由服务端提供出来的]。 前后端分离【把前端的界面效果(html，css，js分离到另一个服务端，python服务端只需要返回数据即可)】 前端形成一个独立的网站, 服务端构成一个独立的网站 2. api接口应用程序编程接口(Application Programming Interface, API接口), 就是应用程序对外提供了一个操作数据的入口, 这个入口可以是一个函数或类方法, 也可以是一个url地址或者一个网络地址. 当客户端调用这个入口, 应用程序则会执行对应代码操作, 给客户端完成相对应的功能. 当然, api接口在工作中是比较常见的开发内容, 有时候, 我们会调用其他人编写的api接口, 有时候, 我们也需要提供api接口给其他人操作. 由此就会带来一个问题, api接口往往都是一个函数、类方法、或者url或其他网络地址, 不断是哪一种, 当api接口编写过程中, 我们都要考虑一个问题就是这个接口应该怎么编写? 接口怎么写的更加容易维护和清晰, 这就需要大家在调用或者编写api接口的时候要有一个明确的编写规范!!! 为了在团队内部形成共识、防止个人习惯差异引起的混乱, 我们都需要找到一种大家都觉得很好的接口实现规范, 而且这种规范能够让后端写的接口, 用途一目了然, 减少客户端和服务端双方之间的合作成本. 目前市面上大部分公司开发人员使用的接口实现规范主要有:restful、RPC. restful: 翻译成中文: 资源状态转换.(表征性状态转移) 把服务端提供的所有的数据&#x2F;文件都看成资源, 那么通过api接口请求数据的操作, 本质上来说就是对资源的操作了. 因此, restful中要求, 我们把当前接口对外提供哪种资源进行操作, 就把资源的名称写在url地址. web开发中操作资源, 最常见的最通用的无非就是增删查改, 所以restful要求在地址栏中声明要操作的资源是什么. 然后通过http请求动词来说明对该资源进行哪一种操作 POST http://www.xxx.com/api/students/ 添加学生数据 GET http://www.xxx.com/api/students/ 获取所有学生 GET http://www.xxx.com/api/students/&#x2F; 获取id&#x3D;pk的学生 DELETE http://www.xxx.com/api/students/&#x2F; 删除id&#x3D;pk的一个学生 PUT http://www.xxx.com/api/students/&#x2F; 修改一个学生的全部信息 [id, name, sex, age, ] PATCH http://www.xxx.com/api/students/&#x2F; 修改一个学生的部分信息[age] 也就是说, 我们仅需要通过url地址上的资源名称结合HTTP请求动作, 就可以说明当前api接口的功能是什么了.restful是以资源为主的api接口规范, 体现在地址上就是资源就是以名词表达. 3. RESTful API规范 REST全称是Representational State Transfer, 中文意思是表述(编者注: 通常译为表征)性状态转移. 它首次出现在2000年Roy Fielding的博士论文中. RESTful是一种专门为Web 开发而定义API接口的设计风格, 尤其适用于前后端分离的应用模式中. 这种风格的理念认为后端开发任务就是提供数据的, 对外提供的是数据资源的访问接口, 所以在定义接口时, 客户端访问的URL路径就表示这种要操作的数据资源. 而对于数据资源分别使用POST、DELETE、GET、UPDATE等请求动作来表达对数据的增删查改. GET &#x2F;students 获取所有学生 请求方法 请求地址 后端操作 POST &#x2F;students 增加学生 GET &#x2F;students&#x2F; 获取编号为pk的学生 PUT &#x2F;students&#x2F; 修改编号为pk的学生 DELETE &#x2F;students&#x2F; 删除编号为pk的学生 restful规范是一种通用的规范, 不限制语言和开发框架的使用. 事实上, 我们可以使用任何一门语言, 任何一个框架都可以实现符合restful规范的API接口. 参考文档:http://www.runoob.com/w3cnote/restful-architecture.html 幂等性接口实现过程中, 会存在幂等性. 所谓幂等性是指代客户端发起多次同样请求时, 是否对于服务端里面的资源产生不同结果. 如果多次请求, 服务端结果还是一样, 则属于幂等接口, 如果多次请求, 服务端产生结果是不一样的, 则属于非幂等接口. 请求方式 是否幂等 是否安全 GET 幂等 安全 POST 不幂等 不安全 PUT&#x2F;PATCH 幂等 不安全 DELETE 幂等 不安全 4. Django的CBV之前我们用的Django的视图函数叫FBV(也就是函数型视图函数), 这里我们来试试CBV(类视图函数)的写法. 类视图函数可以让代码看起来更简洁, 用起来更方便. 4.1 CBV的简单使用视图类: 1234567class BookView(View): def get(self, request): return HttpResponse(&quot;get请求book&quot;) def post(self, request): return HttpResponse(&quot;post请求book&quot;) 路由: 123urlpatterns = [ path(&#x27;book/&#x27;, views.BookView.as_view()),] 4.2 CBV的源码解析4.2.1 面向对象知识点回顾(1)继承12345678910111213141516171819class Animal(object): def __init__(self, name, age): self.name = name self.age = age self.sleep() # 一定要明确self是谁 def sleep(self): print(&quot;sleeping...&quot;)class Dog(Animal): def wangwang(self): print(&quot;旺旺叫&quot;) def sleep(self): print(&quot;仰天睡...&quot;)d = Dog(&quot;alex&quot;, 23) (2)反射123456789101112131415161718192021class Animal(object): def __init__(self, name, age, init_func_str): self.name = name self.age = age func = getattr(self, init_func_str) func() def sleep(self): print(&quot;sleeping...&quot;)class Dog(Animal): def wangwang(self): print(&quot;旺旺叫&quot;) def sleep(self): print(&quot;仰天睡...&quot;)d = Dog(&quot;alex&quot;, 23, &quot;sleep&quot;) 4.2.2 CBV的源码解析123456789101112131415161718192021222324252627282930313233&#x27;&#x27;&#x27;&#x27;class BookView(View): def get(self, request): return HttpResponse(&quot;get请求book&quot;) def post(self, request): return HttpResponse(&quot;post请求book&quot;) class View: @classonlymethod def as_view(cls): def view(request): self = cls() return self.dispatch(request, *args, **kwargs) return view def dispatch(self, request): handler = getattr(self, request.method.lower()) # 按请求方式分发 return handler(request, *args, **kwargs)# 路由path(&#x27;book/&#x27;, views.BookView.as_view()),path(&#x27;book/&#x27;, View.view),# 一旦用户发起请求，比如get请求访问/book/，得到的是get方法的响应结果get请求访问/book/ =&gt; view() =&gt; dispatch() =&gt; get() &#x27;&#x27;&#x27;&#x27; 5. Django_Rest_Framework核心思想: 大量缩减编写api接口的代码 Django REST framework是一个建立在Django基础之上的Web 应用开发框架, 可以快速的开发REST API接口应用. 在REST framework中, 提供了序列化器Serialzier的定义, 可以帮助我们简化序列化与反序列化的过程, 不仅如此, 还提供丰富的类视图、扩展类、视图集来简化视图的编写工作. REST framework还提供了认证、权限、限流、过滤、分页、接口文档等功能支持. REST framework提供了一个API 的Web可视化界面来方便查看测试接口. 中文文档:https://q1mi.github.io/Django-REST-framework-documentation/#django-rest-framework github: https://github.com/encode/django-rest-framework/tree/master 特点: 提供了定义序列化器Serializer的方法, 可以快速根据 Django ORM 或者其它库自动序列化&#x2F;反序列化; 提供了丰富的类视图、Mixin扩展类, 简化视图的编写; 丰富的定制层级: 函数视图、类视图、视图集合到自动生成 API, 满足各种需要; 多种身份认证和权限认证方式的支持; [jwt] 内置了限流系统; 直观的 API web 界面; 【方便我们调试开发api接口】 可扩展性, 插件丰富 6. 环境安装与配置DRF需要以下依赖: Python (3.5 以上) Django (2.2 以上) DRF是以Django子应用的方式提供的, 所以我们可以直接利用已有的Django环境而无需从新创建.(若没有Django环境, 需要先创建环境安装Django) 前提是已经安装了django, 建议安装在虚拟环境 1234# conda create -n drfdemo python=3.8# pip install django==3.2.4 -i https://pypi.douban.com/simplepip install djangorestframework -i https://pypi.douban.com/simple 7. DRF的APIView视图7.1 APIView的简单使用1rest_framework.views.APIView APIView 是REST framework提供的所有视图的基类, 继承自Django的 View 父类. 相较于CBV, 路由不变, 视图不变, 继承类改为 APIView 1234567class BookView(APIView): def get(self, request): return HttpResponse(&quot;APIView:get请求book&quot;) def post(self, request): return HttpResponse(&quot;APIView:post请求book&quot;) 7.2 APIView的源码解析1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class BookView(APIView): def get(self, request): return HttpResponse(&quot;APIView:get请求book&quot;) def post(self, request): return HttpResponse(&quot;APIView:post请求book&quot;)class APIView(View): @classmethod def as_view(cls): view = super().as_view() view.cls = cls return view def dispatch(self, request): # （1）构建新的request对象 request = self.initialize_request(request, *args, **kwargs) self.request = request # （2）认证、权限、限流组件三件套 self.initial(request, *args, **kwargs) # （3）分发 handler = getattr(self, request.method.lower()) # 按请求方式分发 return handler(request, *args, **kwargs) class View: @classonlymethod def as_view(cls): def view(request): self = cls() return self.dispatch(request, *args, **kwargs) return view def dispatch(self, request): handler = getattr(self, request.method.lower()) # 按请求方式分发 return handler(request, *args, **kwargs) # 路由path(&#x27;book/&#x27;, views.BookView.as_view()),path(&#x27;book/&#x27;, View.view),# 一旦用户发起请求，比如get请求访问/book/,依然得到get方法的响应get请求访问/book/ =&gt; View.view() =&gt; APIView.dispatch() =&gt; BookView.get() APIView 与 View 的不同之处在于: 传入到视图方法中的是REST framework的 Request 对象, 而不是Django的 HttpRequeset 对象; 视图方法可以返回REST framework的 Response 对象, 视图会为响应数据设置(render)符合前端期望要求的格式; 任何 APIException 异常都会被捕获到, 并且处理成合适格式的响应信息返回给客户端; 重新声明了一个新的as_views方法并在dispatch()进行路由分发前, 会对请求的客户端进行身份认证、权限检查、流量控制. DRF在django原有的基础上, 新增了一个request对象继承到了APIVIew视图类, 并在django原有的HttpResponse响应类的基础上实现了一个子类rest_framework.response. Response响应类. 这两个类, 都是基于内容协商来完成数据的格式转换的. request-&gt;parser-&gt;识别客户端请求头中的Content-Type来完成数据转换成-&gt;类字典(QueryDict, 字典的子类) response-&gt;renderer-&gt;识别客户端请求头的”Accept”来提取客户单期望的返回数据格式, -&gt; 转换成客户端的期望格式数据 7.1、请求REST framework 传入视图的request对象不再是Django默认的HttpRequest对象, 而是REST framework提供的扩展了HttpRequest类的Request类的对象. REST framework 提供了Parser解析器, 在接收到请求后会自动根据Content-Type指明的请求数据类型(如JSON、表单等)将请求数据进行parse解析, 解析为类字典[QueryDict]对象保存到Request对象中. Request对象的数据是自动根据前端发送数据的格式进行解析之后的结果. 无论前端发送的哪种格式的数据, 我们都可以以统一的方式读取数据. 7.1.1、常用属性1).datarequest.data 返回解析之后的请求体数据. 类似于Django中标准的 request.POST 和 request.FILES 属性, 但提供如下特性: 包含了解析之后的文件和非文件数据 包含了对POST、PUT、PATCH请求方式解析后的数据 利用了REST framework的parsers解析器, 不仅支持表单类型数据, 也支持JSON数据 2).query_paramsrequest.query_params 与Django标准的 request.GET 相同, 只是更换了更正确的名称而已. 3)request._request获取django封装的Request对象 7.1.2、基本使用视图代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from django.views import Viewfrom django.http.response import HttpResponsefrom django.http.request import HttpRequestfrom django.core.handlers.wsgi import WSGIRequestclass ReqView(View): def get(self,request): print(request) return HttpResponse(&quot;ok&quot;)&quot;&quot;&quot;默认情况下, 编写视图类时，如果继承的是django内置的django.view.View视图基类，则视图方法中得到的request对象，是django默认提供的django.core.handlers.wsgi.WSGIRequestWSGIRequest这个请求处理对象，无法直接提供的关于json数据数据处理。在编写api接口时很不方便，所以drf为了简写这块内容，在原来的HttpRequest的基础上面，新增了一个Request对象这个Request对象是单独声明的和原来django的HttpRequest不是父子关系。同时注意： 要使用drf提供的Request请求处理对象，必须在编写视图类时继承drf提供的视图基类 from rest_framework.views import APIView 如果使用drf提供的视图基类APIView编写类视图，则必须使用来自drf提供的Request请求对象和Response响应对象&quot;&quot;&quot;from rest_framework.views import APIViewfrom rest_framework.response import Responsefrom rest_framework import statusclass ReqAPIView(APIView): def get(self,request): # rest_framework.request.Request对象 print(request) # &lt;rest_framework.request.Request: GET &#x27;/req/req2?name=xiaoming&amp;age=17&amp;lve=swim&amp;lve=code&#x27;&gt; # 获取查询字符串 print(request.query_params) # 没有参数情况下： &lt;QueryDict: &#123;&#125;&gt; # 有参数的情况下： &lt;QueryDict: &#123;&#x27;name&#x27;: [&#x27;xiaoming&#x27;], &#x27;age&#x27;: [&#x27;17&#x27;], &#x27;lve&#x27;: [&#x27;swim&#x27;, &#x27;code&#x27;]&#125;&gt; # 所以，request.query_params的返回值操作和原来在django里面是一模一样的 print(request.query_params.get(&quot;name&quot;)) # xiaoming print(request.query_params.getlist(&quot;lve&quot;)) # [&#x27;swim&#x27;, &#x27;code&#x27;] return Response(&quot;ok&quot;) def post(self, request): # 获取请求体 print(request.data) # &#123;&#x27;name&#x27;: &#x27;xiaoming&#x27;, &#x27;age&#x27;: 16, &#x27;lve&#x27;: [&#x27;swim&#x27;, &#x27;code&#x27;]&#125; &quot;&quot;&quot;直接从请求体中提取数据转 # 客户端如果上传了json数据，直接返回字典 &#123;&#x27;name&#x27;: &#x27;灰太狼&#x27;, &#x27;age&#x27;: 20, &#x27;sex&#x27;: 1, &#x27;classmate&#x27;: &#x27;301&#x27;, &#x27;description&#x27;: &#x27;我还会再回来的~&#x27;&#125; # 客户端如果上传了表单数据，直接返回QueryDict &lt;QueryDict: &#123;&#x27;name&#x27;: [&#x27;xiaohui&#x27;], &#x27;age&#x27;: [&#x27;18&#x27;]&#125;&gt; &quot;&quot;&quot; print(request.FILES) # 获取上传文件列表 # 要获取django原生提供的HttpRequest对象，可以通过request._request来获取到 print(request._request.META.get(&quot;Accept&quot;)) # 当值为None时，drf默认在响应数据时按json格式返回 # response = Response(data=&quot;not ok&quot;, status=204, headers=&#123;&quot;Company&quot;:&quot;Oldboy&quot;&#125;) response = Response(data=&quot;not ok&quot;, status=status.HTTP_400_BAD_REQUEST, headers=&#123;&quot;Company&quot;:&quot;Oldboy&quot;&#125;) return response 7.2、响应1rest_framework.response.Response REST framework提供了一个响应类 Response , 使用该类构造响应对象时, 响应的具体数据内容会被转换(render渲染器)成符合前端需求的类型. REST framework提供了 Renderer 渲染器, 用来根据请求头中的 Accept (接收数据类型声明)来自动转换响应数据到对应格式. 如果前端请求中未进行Accept声明, 则会采用Content-Type方式处理响应数据, 我们可以通过配置来修改默认响应格式. 可以在rest_framework.settings查找所有的drf默认配置项 123456REST_FRAMEWORK = &#123; &#x27;DEFAULT_RENDERER_CLASSES&#x27;: ( # 默认响应渲染类 &#x27;rest_framework.renderers.JSONRenderer&#x27;, # json渲染器，返回json数据 &#x27;rest_framework.renderers.BrowsableAPIRenderer&#x27;, # 浏览器API渲染器，返回调试界面 )&#125; 7.2.1 构造方式1Response(data, status=None, template_name=None, headers=None, content_type=None) drf的响应处理类和请求处理类不一样, Response就是django的HttpResponse响应处理类的子类. data 数据不要是render处理之后的数据, 只需传递python的内建类型数据即可, REST framework会使用 renderer 渲染器处理 data . data 不能是复杂结构的数据, 如Django的模型类对象, 对于这样的数据我们可以使用 Serializer 序列化器序列化处理后(转为了Python字典类型)再传递给 data 参数. 参数说明: data : 为响应准备的序列化处理后的数据; status : 状态码, 默认200; template_name : 模板名称, 如果使用 HTMLRenderer 时需指明; headers : 用于存放响应头信息的字典; content_type : 响应数据的Content-Type, 通常此参数无需传递, REST framework会根据前端所需类型数据来设置该参数 7.2.2 response对象的属性 .data: 传给response对象的序列化后, 但尚未render处理的数据 .status_code: 状态码的数字 .content: 经过render处理后的响应数据 7.2.3 状态码为了方便设置状态码, REST framewrok在 rest_framework.status 模块中提供了常用http状态码的常量. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#x27;&#x27;&#x27; 1）信息告知 - 1xx &#x27;&#x27;&#x27;HTTP_100_CONTINUEHTTP_101_SWITCHING_PROTOCOLS&#x27;&#x27;&#x27; 2）成功 - 2xx&#x27;&#x27;&#x27;HTTP_200_OKHTTP_201_CREATEDHTTP_202_ACCEPTEDHTTP_203_NON_AUTHORITATIVE_INFORMATIONHTTP_204_NO_CONTENTHTTP_205_RESET_CONTENTHTTP_206_PARTIAL_CONTENTHTTP_207_MULTI_STATUS&#x27;&#x27;&#x27; 3）重定向 - 3xx&#x27;&#x27;&#x27;HTTP_300_MULTIPLE_CHOICESHTTP_301_MOVED_PERMANENTLYHTTP_302_FOUNDHTTP_303_SEE_OTHERHTTP_304_NOT_MODIFIEDHTTP_305_USE_PROXYHTTP_306_RESERVEDHTTP_307_TEMPORARY_REDIRECT&#x27;&#x27;&#x27; 4）客户端错误 - 4xx&#x27;&#x27;&#x27;HTTP_400_BAD_REQUESTHTTP_401_UNAUTHORIZEDHTTP_402_PAYMENT_REQUIREDHTTP_403_FORBIDDENHTTP_404_NOT_FOUNDHTTP_405_METHOD_NOT_ALLOWEDHTTP_406_NOT_ACCEPTABLEHTTP_407_PROXY_AUTHENTICATION_REQUIREDHTTP_408_REQUEST_TIMEOUTHTTP_409_CONFLICTHTTP_410_GONEHTTP_411_LENGTH_REQUIREDHTTP_412_PRECONDITION_FAILEDHTTP_413_REQUEST_ENTITY_TOO_LARGEHTTP_414_REQUEST_URI_TOO_LONGHTTP_415_UNSUPPORTED_MEDIA_TYPEHTTP_416_REQUESTED_RANGE_NOT_SATISFIABLEHTTP_417_EXPECTATION_FAILEDHTTP_422_UNPROCESSABLE_ENTITYHTTP_423_LOCKEDHTTP_424_FAILED_DEPENDENCYHTTP_428_PRECONDITION_REQUIREDHTTP_429_TOO_MANY_REQUESTSHTTP_431_REQUEST_HEADER_FIELDS_TOO_LARGEHTTP_451_UNAVAILABLE_FOR_LEGAL_REASONS&#x27;&#x27;&#x27; 5）服务器错误 - 5xx&#x27;&#x27;&#x27;HTTP_500_INTERNAL_SERVER_ERRORHTTP_501_NOT_IMPLEMENTEDHTTP_502_BAD_GATEWAYHTTP_503_SERVICE_UNAVAILABLEHTTP_504_GATEWAY_TIMEOUTHTTP_505_HTTP_VERSION_NOT_SUPPORTEDHTTP_507_INSUFFICIENT_STORAGEHTTP_511_NETWORK_AUTHENTICATION_REQUIRED 8. 序列化器-Serializer作用: 1. 序列化,序列化器会把模型对象转换成字典,经过response以后变成json字符串 2. 反序列化,把客户端发送过来的数据,经过request以后变成字典,序列化器可以把字典转成模型 3. 反序列化,完成数据校验功能 8.1 定义序列化器Django REST framework中的Serializer使用类来定义, 须继承自rest_framework.serializers. Serializer. 接下来, 为了方便演示序列化器的使用, 我们先创建一个新的子应用sers 1python manage.py startapp sers 我们创建几个图书相关模型 12345678from django.db import models&#x27;&#x27;&#x27; Create your models here.&#x27;&#x27;&#x27;class Book(models.Model): title = models.CharField(max_length=32,verbose_name=&quot;书籍名称&quot;) price = models.IntegerField(verbose_name=&quot;价格&quot;) pub_date = models.DateField(verbose_name=&quot;出版日期&quot;) 我们想为Book模型类提供一个序列化器, 可以定义如下: 123456from rest_framework import serializersclass BookSerializer(serializers.Serializer): title = serializers.CharField() price = serializers.IntegerField() pub_date = serializers.DateField() **注意:serializer不是只能为数据库模型类定义, 也可以为非数据库模型类的数据定义.**serializer是独立于数据库之外的存在. 8.2 创建Serializer对象定义好Serializer类后, 就可以创建Serializer对象了. Serializer的构造方法为: 1Serializer(instance=None, data=empty, **kwarg) 说明: 1)用于序列化时, 将模型类对象传入instance参数 2)用于反序列化时, 将要被反序列化的数据传入data参数 3)除了instance和data参数外, 在构造Serializer对象时, 还可通过context参数额外添加数据, 如 1serializer = AccountSerializer(account, context=&#123;&#x27;request&#x27;: request&#125;) 通过context参数附加的数据, 可以通过Serializer对象的context属性获取. 使用序列化器的时候一定要注意, 序列化器声明了以后, 不会自动执行, 需要我们在视图中进行调用才可以. 序列化器无法直接接收数据, 需要我们在视图中创建序列化器对象时把使用的数据传递过来. 序列化器的字段声明类似于我们前面使用过的表单系统. 开发restful api时, 序列化器会帮我们把模型数据转换成字典. drf提供的视图会帮我们把字典转换成json, 或者把客户端发送过来的数据转换字典. 8.3 序列化器的使用序列化器的使用分两个阶段: 处理客户端请求时，使用序列化器可以完成对数据的反序列化。 处理服务器响应时，使用序列化器可以完成对数据的序列化。 8.3.1 序列化(1)基本序列化《1》 先查询出一个学生对象 12from sers.models import Bookbook = Book.objects.get(pk=1) 《2》 构造序列化器对象 12from .serializers import BookSerializerbookSer = BookSerializer(instance=book) 《3》获取序列化数据 通过data属性可以获取序列化后的数据 12bookSer.data # &#123;&#x27;title&#x27;: &#x27;乱世佳人&#x27;, &#x27;price&#x27;: 335, &#x27;pub_date&#x27;: &#x27;2012-12-12&#x27;&#125; 路由视图代码: 123456789# urls.pypath(&quot;sers/&quot;, include(&quot;sers.urls&quot;)), # sers.urlspath(&#x27;books/(\\d+)&#x27;, BookView.as_view()), 1234567891011from rest_framework.response import Responsefrom rest_framework.views import APIViewfrom .models import Bookfrom .sers import BookSerializerclass BookView(APIView): def get(self, request,id): book = Book.objects.get(pk=id) bs = BookSerializer(instance=book) return Response(bs.data) 《5》如果要被序列化的是包含多条数据的查询集QuerySet, 可以通过添加many&#x3D;True参数补充说明 1234567class BookView(APIView): def get(self, request): # book = Book.objects.get(pk=1) books = Book.objects.all() bs = BookSerializer(instance=books, many=True) return Response(bs.data) 8.3.2 反序列化(1)数据验证使用序列化器进行反序列化时, 需要对数据进行验证后, 才能获取验证成功的数据或保存成模型类对象. 在获取反序列化的数据前, 必须调用**is_valid()**方法进行验证, 验证成功返回True, 否则返回False. 验证失败, 可以通过序列化器对象的errors属性获取错误信息, 返回字典, 包含了字段和字段的错误. 如果是非字段错误, 可以通过修改REST framework配置中的NON_FIELD_ERRORS_KEY来控制错误字典中的键名. 验证成功, 可以通过序列化器对象的validated_data属性获取数据. 在定义序列化器时, 指明每个字段的序列化类型和选项参数, 本身就是一种验证行为. 通过构造序列化器对象, 并将要反序列化的数据传递给data构造参数, 进而进行验证. 12345678from sers.sers import BookSerializerbs = BookSerializer(data=&#123;&quot;title&quot;:&quot;小王子&quot;,&quot;price&quot;:100&#125;)bs.is_valid() # 必须先要is_valid,才会有bs.validated_data和bs.errorsFalsebs.validated_data&#123;&#125;bs.errors&#123;&#x27;pub_date&#x27;: [ErrorDetail(string=&#x27;This field is required.&#x27;, code=&#x27;required&#x27;)]&#125; 可以设置required&#x3D;False让校验字段可以为空! is_valid()方法还可以在验证失败时抛出异常serializers. ValidationError, 可以通过传递raise_exception&#x3D;True参数开启, REST framework接收到此异常, 会向前端返回HTTP 400 Bad Request响应. 1# Return a 400 response if the data was invalid.serializer.is_valid(raise_exception=True) 如果觉得这些还不够, 需要再补充定义验证行为, 可以使用以下三种方法: (1) validate_字段名对 &lt;field_name&gt; 字段进行验证, 如 123456789class BookSerializer(serializers.Serializer): title = serializers.CharField(max_length=32) price = serializers.IntegerField(required=True) pub_date = serializers.DateField(required=True) def validate_title(self, value): if &#x27;django&#x27; not in value.lower(): raise serializers.ValidationError(&quot;图书不是关于Django的&quot;) return value 测试 123456from sers.sers import BookSerializerbs = BookSerializer(data=&#123;&quot;title&quot;:&quot;小王子&quot;,&quot;price&quot;:100&#125;)bs.is_valid()Falsebs.errors&#123;&#x27;title&#x27;: [ErrorDetail(string=&#x27;图书不是关于Django的&#x27;, code=&#x27;invalid&#x27;)], &#x27;pub_date&#x27;: [ErrorDetail(string=&#x27;This field is required.&#x27;, code=&#x27;required&#x27;)]&#125; 还有一种写法: 1234567891011def title_django(self, value): if &#x27;django&#x27; not in value.lower(): raise serializers.ValidationError(&quot;图书不是关于Django的&quot;) return valueclass BookSerializer(serializers. Serializer): title = serializers.CharField(max_length=32,validators=[title_django,]) ... (2) validate在序列化器中需要同时对多个字段进行比较验证时，可以定义validate方法来验证，如 123456789101112131415161718192021class BookSerializer(serializers.Serializer): title = serializers.CharField(max_length=32) price = serializers.IntegerField(required=False) pub_date = serializers.DateField(required=False) bread = serializers.IntegerField(label=&#x27;阅读量&#x27;, max_value=2147483647, min_value=-2147483648, required=False) bcomment = serializers.IntegerField(label=&#x27;评论量&#x27;, max_value=2147483647, min_value=-2147483648, required=False) def validate_title(self, value): if &#x27;django&#x27; not in value.lower(): raise serializers.ValidationError(&quot;图书不是关于Django的&quot;) return value def validate(self, data): bread = data.get(&quot;bread&quot;) bcomment = data.get(&quot;bcomment&quot;) if bread &lt; bcomment: raise serializers.ValidationError(&#x27;阅读量小于评论量&#x27;) return data 测试 12345bs = BookSerializer(data=&#123;&quot;title&quot;:&quot;Django深入浅出&quot;,&quot;bread&quot;:100,&quot;bcomment&quot;:200,&quot;publish_id&quot;:1&#125;)bs.is_valid()Falsebs.errors&#123;&#x27;non_field_errors&#x27;: [ErrorDetail(string=&#x27;阅读量小于评论量&#x27;, code=&#x27;invalid&#x27;)]&#125; (2)反序列化-保存数据前面的验证数据成功后, 我们可以使用序列化器来完成数据反序列化的过程. 这个过程可以把数据转成模型类对象. 可以通过实现create()和update()两个方法来实现. 1234567891011class BookSerializer(serializers.Serializer): &quot;&quot;&quot;图书数据序列化器&quot;&quot;&quot; ... def create(self, validated_data): &quot;&quot;&quot;新建&quot;&quot;&quot; return instance def update(self, instance, validated_data): return instance 实现了上述两个方法后, 在反序列化数据的时候, 就可以通过save()方法返回一个数据对象实例了 1book = serializer.save() 1、如果创建序列化器对象的时候, 没有传递instance实例, 则调用save()方法的时候, create()被调用 2、相反, 如果传递了instance实例, 则调用save()方法的时候, update()被调用. 1234567891011from django.urls import path, re_path , includefrom sers.views import BookView, BookDetailViewurlpatterns = [ path(&#x27;books/&#x27;, BookView.as_view()), re_path(&#x27;books/(?P&lt;pk&gt;\\d+)/&#x27;, BookDetailView.as_view()),] 123# 序列化器# 声明序列化器，所有的序列化器都要直接或者间接继承于 Serializer# 其中，ModelSerializer是Serializer的子类，ModelSerializer在Serializer的基础上进行了代码简化 (3) 附加说明 在对序列化器进行save()保存时, 可以额外传递数据, 这些数据可以在create()和update()中的validated_data参数获取到 1# request.user 是django中记录当前登录用户的模型对象serializer.save(owner=request.user) 2)默认序列化器必须传递所有required的字段, 否则会抛出验证异常. 但是我们可以使用partial参数来允许部分字段更新 1# Update `comment` with partial dataserializer = CommentSerializer(comment, data=&#123;&#x27;content&#x27;: u&#x27;foo bar&#x27;&#125;, partial=True) 8.3.3、基于APIView的接口实现路由: 1234urlpatterns = [ path(&#x27;book/&#x27;, views.BookView.as_view()), re_path(&#x27;book/(\\d+)&#x27;, views.BookDetailView.as_view()),] 视图: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&#x27;&#x27;&#x27; Create your views here.&#x27;&#x27;&#x27;from rest_framework.views import APIView&#x27;&#x27;&#x27;设计增删改查查接口&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;序列化器&#x27;&#x27;&#x27;from rest_framework import serializersfrom rest_framework.response import Responsefrom .models import Bookclass BookSerializer(serializers.Serializer): title = serializers.CharField() price = serializers.IntegerField() pub_date = serializers.DateField() def create(self, validated_data): new_book = Book.objects.create(**validated_data) return new_book def update(self, instance, validated_data): Book.objects.filter(pk=instance.pk).update(**validated_data) instance = Book.objects.get(pk=instance.pk) return instance&#x27;&#x27;&#x27;视图&#x27;&#x27;&#x27;class BookView(APIView): def get(self, request): books = Book.objects.all() bs = BookSerializer(instance=books, many=True) return Response(bs.data) def post(self, request): bs = BookSerializer(data=request.data) if bs.is_valid(): bs.save() return Response(bs.data) else: return Response(bs.errors)class BookDetailView(APIView): def get(self, request, pk): book = Book.objects.get(pk=pk) bs = BookSerializer(instance=book) return Response(bs.data) def put(self, request, pk): instance = Book.objects.get(pk=pk) bs = BookSerializer(instance=instance, data=request.data) if bs.is_valid(): bs.save() return Response(bs.data) else: return Response(bs.errors) def delete(self, request, pk): Book.objects.get(pk=pk).delete() return Response() 8.4 模型类序列化器如果我们想要使用序列化器对应的是Django的模型类, DRF为我们提供了ModelSerializer模型类序列化器来帮助我们快速创建一个Serializer类. ModelSerializer与常规的Serializer相同, 但提供了: 基于模型类自动生成一系列字段 基于模型类自动为Serializer生成validators, 比如unique_together 包含默认的create()和update()的实现 (1)定义比如我们创建一个BookInfoSerializer 12345678class BookSerializer(serializers. ModelSerializer): &quot;&quot;&quot;图书数据序列化器&quot;&quot;&quot; class Meta: model = Book fields = &#x27;__all__&#x27; model 指明参照哪个模型类 fields 指明为模型类的哪些字段生成 我们可以在python manage.py shell中查看自动生成的BookSerializer的具体实现。 123456789101112131415161718192021222324252627282930313233343536373839404142class BookSerializer(serializers.ModelSerializer): &quot;&quot;&quot;图书数据序列化器&quot;&quot;&quot; class Meta: model = Book fields = &#x27;__all__&#x27;&#x27;&#x27;&#x27;视图&#x27;&#x27;&#x27;class BookView(APIView): def get(self, request): books = Book.objects.all() bs = BookSerializer(instance=books, many=True) return Response(bs.data) def post(self, request): bs = BookSerializer(data=request.data) if bs.is_valid(): bs.save() return Response(bs.data) else: return Response(bs.errors)class BookDetailView(APIView): def get(self, request, pk): book = Book.objects.get(pk=pk) bs = BookSerializer(instance=book) return Response(bs.data) def put(self, request, pk): instance = Book.objects.get(pk=pk) bs = BookSerializer(instance=instance, data=request.data) if bs.is_valid(): bs.save() return Response(bs.data) else: return Response(bs.errors) def delete(self, request, pk): Book.objects.get(pk=pk).delete() return Response() 注意, 只有序列化器改为BookSerializer, 其他部分都没动 (2)指定字段 使用fields来明确字段, __all__ 表名包含所有字段, 也可以写明具体哪些字段, 如 12345class BookSerializer(serializers.ModelSerializer): &quot;&quot;&quot;图书数据序列化器&quot;&quot;&quot; class Meta model = BookInfo fields = (&#x27;id&#x27;, &#x27;title&#x27;, &#x27;pub_date&#x27;) 使用exclude可以明确排除掉哪些字段 12345class BookSerializer(serializers.ModelSerializer): &quot;&quot;&quot;图书数据序列化器&quot;&quot;&quot; class Meta: model = Book exclude = (&#x27;pub_date&#x27;,) 指明只读字段 可以通过read_only_fields指明只读字段, 即仅用于序列化输出的字段 123456class BookSerializer(serializers.ModelSerializer): &quot;&quot;&quot;图书数据序列化器&quot;&quot;&quot; class Meta: model = Book fields = (&#x27;id&#x27;, &#x27;title&#x27;, &#x27;pub_date&#x27;， &#x27;bread&#x27;, &#x27;bcomment&#x27;) read_only_fields = (&#x27;id&#x27;, &#x27;bread&#x27;, &#x27;bcomment&#x27;) (3)添加额外参数我们可以使用extra_kwargs参数为ModelSerializer添加或修改原有的选项参数 12345678910111213141516class BookSerializer(serializers.ModelSerializer): &quot;&quot;&quot;图书数据序列化器&quot;&quot;&quot; class Meta: model = Book fields = (&#x27;id&#x27;, &#x27;title&#x27;, &#x27;pub_date&#x27;) extra_kwargs = &#123; &#x27;bread&#x27;: &#123;&#x27;min_value&#x27;: 0, &#x27;required&#x27;: True&#125;, &#x27;bcomment&#x27;: &#123;&#x27;min_value&#x27;: 0, &#x27;required&#x27;: True&#125;, &#125;&#x27;&#x27;&#x27;BookSerializer(): id = IntegerField(label=&#x27;ID&#x27;, read_only=True) title = CharField(label=&#x27;书籍名称&#x27;, max_length=32) pub_date = DateField(label=&#x27;出版日期&#x27;)&#x27;&#x27;&#x27; 参考文章 9. 视图Django REST framwork 提供的视图的主要作用: 控制序列化器的执行(检验、保存、转换数据) 控制数据库模型的操作 REST framework 提供了众多的通用视图基类与扩展类, 以简化视图的编写. 9.1、GenericAPIView[通用视图类]通用视图类主要作用就是把视图中的独特的代码抽取出来, 让视图方法中的代码更加通用, 方便把通用代码进行简写. 1rest_framework.generics.GenericAPIView 继承自 APIView , 主要增加了操作序列化器和数据库查询的方法, 作用是为下面Mixin扩展类的执行提供方法支持. 通常在使用时, 可搭配一个或多个Mixin扩展类. 提供的关于序列化器使用的属性与方法 (1)get_serializer_class(self)当出现一个视图类中调用多个序列化器时, 那么可以通过条件判断在get_serializer_class方法中通过返回不同的序列化器类名就可以让视图方法执行不同的序列化器对象了. 返回序列化器类, 默认返回 serializer_class , 可以重写 (2)get_serializer(self, *args, **kwargs)返回序列化器对象, 主要用来提供给Mixin扩展类使用, 如果我们在视图中想要获取序列化器对象, 也可以直接调用此方法. 注意, 该方法在提供序列化器对象的时候, 会向序列化器对象的context属性补充三个数据:request、format、view, 这三个数据对象可以在定义序列化器时使用. request 当前视图的请求对象 view 当前请求的类视图对象 format 当前请求期望返回的数据格式 (3)get_queryset(self)返回视图使用的查询集, 主要用来提供给Mixin扩展类使用, 是列表视图与详情视图获取数据的基础, 默认返回 queryset 属性, 可以重写, 例如: 123def get_queryset(self): user = self.request.user return user.accounts.all() (4)get_object(self)返回详情视图所需的模型类数据对象, 主要用来提供给Mixin扩展类使用. 在试图中可以调用该方法获取详情信息的模型类对象. 若详情访问的模型类对象不存在, 会返回404. 该方法会默认使用APIView提供的check_object_permissions方法检查当前对象是否有权限被访问. 举例: 123456789&#x27;&#x27;&#x27;url(r&#x27;^books/(?P&lt;pk&gt;\\d+)/$&#x27;, views.BookDetailView.as_view()),&#x27;&#x27;&#x27;class BookDetailView(GenericAPIView): queryset = BookInfo.objects.all() serializer_class = BookInfoSerializer def get(self, request, pk): book = self.get_object() # get_object()方法根据pk参数查找queryset中的数据对象 serializer = self.get_serializer(book) return Response(serializer.data) 其他可以设置的属性 pagination_class 指明分页控制类 filter_backends 指明过滤控制后端 通用视图类的应用 为了方便学习上面的GenericAPIView通用视图类, 我们新建一个子应用. 1python manage.py startapp gen 路由: 1234567urlpatterns = [ path(&#x27;book/&#x27;, views.BookView.as_view()), re_path(&#x27;book/(? P&lt;pk&gt;\\d+)&#x27;, views. BookDetailView.as_view()), ] 视图： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&#x27;&#x27;&#x27;设计增删改查查接口&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;序列化器&#x27;&#x27;&#x27;from rest_framework import serializersfrom rest_framework.response import Responsefrom .models import Bookfrom rest_framework.generics import GenericAPIViewfrom rest_framework import statusclass BookSerializer(serializers.ModelSerializer): &quot;&quot;&quot;图书数据序列化器&quot;&quot;&quot; class Meta: model = Book fields = &#x27;__all__&#x27;class BookView(GenericAPIView): queryset = Book.objects.all() serializer_class = BookSerializer def get(self, request): bs = BookSerializer(instance=self.get_queryset(), many=True) return Response(bs.data) def post(self, request): serializer = self.get_serializer(data=request.data) serializer.is_valid(raise_exception=True) serializer.save() return Response(serializer.data)class BookDetailView(GenericAPIView): queryset = Book.objects.all() serializer_class = BookSerializer def get(self, request, pk): bs = BookSerializer(instance=self.get_object()) return Response(bs.data) def put(self, request, pk): serializer = BookSerializer(instance=self.get_object(), data=request.data) serializer.is_valid(raise_exception=True) serializer.save() return Response(serializer.data) def delete(self, request, pk): self.get_object().delete() return Response(status=status.HTTP_204_NO_CONTENT) 9.2、5个视图扩展类也叫混入类(Mixin). 作用: 提供了几种后端视图(对数据资源进行增删改查)处理流程的实现, 如果需要编写的视图属于这五种, 则视图可以通过继承相应的扩展类来复用代码, 减少自己编写的代码量. 这五个扩展类需要搭配GenericAPIView通用视图基类, 因为五个扩展类的实现需要调用GenericAPIView提供的序列化器与数据库查询的方法. (1)ListModelMixin列表视图扩展类, 提供 list(request, *args, **kwargs) 方法快速实现列表视图, 返回200状态码. 该Mixin的list方法会对数据进行过滤和分页. 源代码: 1234567891011121314class ListModelMixin: &quot;&quot;&quot; List a queryset. &quot;&quot;&quot; def list(self, request, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) page = self.paginate_queryset(queryset) if page is not None: serializer = self.get_serializer(page, many=True) return self.get_paginated_response(serializer.data) serializer = self.get_serializer(queryset, many=True) return Response(serializer.data) 基于扩展类实现BookView: 1234567891011121314151617from rest_framework import statusfrom rest_framework.generics import GenericAPIViewfrom rest_framework.response import Responsefrom sers.models import Bookfrom .sers import BookSerializerfrom rest_framework.mixins import ListModelMixinclass BookView(GenericAPIView, ListModelMixin): queryset = Book.objects.all() serializer_class = BookSerializer def get(self, request): return self.list(request) （2）CreateModelMixin创建视图扩展类，提供create(request, *args, **kwargs)方法快速实现创建资源的视图，成功返回201状态码。 如果序列化器对前端发送的数据验证失败，返回400错误。 源代码： 12345678910111213141516171819class CreateModelMixin: &quot;&quot;&quot; Create a model instance. &quot;&quot;&quot; def create(self, request, *args, **kwargs): serializer = self.get_serializer(data=request.data) serializer.is_valid(raise_exception=True) self.perform_create(serializer) headers = self.get_success_headers(serializer.data) return Response(serializer.data, status=status.HTTP_201_CREATED, headers=headers) def perform_create(self, serializer): serializer.save() def get_success_headers(self, data): try: return &#123;&#x27;Location&#x27;: str(data[api_settings.URL_FIELD_NAME])&#125; except (TypeError, KeyError): return &#123;&#125; 基于扩展类实现BookView: 1234567891011class BookView(GenericAPIView, ListModelMixin, CreateModelMixin): queryset = Book.objects.all() serializer_class = BookSerializer def get(self, request): return self.list(request) def post(self, request): return self.create(request) （3）RetrieveModelMixin详情视图扩展类，提供retrieve(request, *args, **kwargs)方法，可以快速实现返回一个存在的数据对象。 如果存在，返回200， 否则返回404。 源代码： 12345678class RetrieveModelMixin: &quot;&quot;&quot; Retrieve a model instance. &quot;&quot;&quot; def retrieve(self, request, *args, **kwargs): instance = self.get_object() serializer = self.get_serializer(instance) return Response(serializer.data) 基于扩展类实现BookDetailView: 12345678class BookDetailView(GenericAPIView, RetrieveModelMixin): queryset = Book.objects.all() serializer_class = BookSerializer def get(self, request, pk): return self.retrieve(request, pk) （4）UpdateModelMixin更新视图扩展类，提供update(request, *args, **kwargs)方法，可以快速实现更新一个存在的数据对象。 同时也提供partial_update(request, *args, **kwargs)方法，可以实现局部更新。 成功返回200，序列化器校验数据失败时，返回400错误。 源代码： 1234567891011121314151617181920212223242526class UpdateModelMixin: &quot;&quot;&quot; Update a model instance. &quot;&quot;&quot; def update(self, request, *args, **kwargs): partial = kwargs.pop(&#x27;partial&#x27;, False) instance = self.get_object() serializer = self.get_serializer(instance, data=request.data, partial=partial) serializer.is_valid(raise_exception=True) self.perform_update(serializer) if getattr(instance, &#x27;_prefetched_objects_cache&#x27;, None): &quot;&quot;&quot;If &#x27;prefetch_related&#x27; has been applied to a queryset, we need to forcibly invalidate the prefetch cache on the instance.&quot;&quot;&quot; instance._prefetched_objects_cache = &#123;&#125; return Response(serializer.data) def perform_update(self, serializer): serializer.save() def partial_update(self, request, *args, **kwargs): kwargs[&#x27;partial&#x27;] = True return self.update(request, *args, **kwargs) 基于扩展类实现BookDetailView: 1234567891011class BookDetailView(GenericAPIView, RetrieveModelMixin, UpdateModelMixin): queryset = Book.objects.all() serializer_class = BookSerializer def get(self, request, pk): return self.retrieve(request, pk) def put(self, request, pk): return self.update(request, pk) （5）DestroyModelMixin删除视图扩展类，提供destroy(request, *args, **kwargs)方法，可以快速实现删除一个存在的数据对象。 成功返回204，不存在返回404。 源代码： 1234567891011class DestroyModelMixin: &quot;&quot;&quot; Destroy a model instance. &quot;&quot;&quot; def destroy(self, request, *args, **kwargs): instance = self.get_object() self.perform_destroy(instance) return Response(status=status.HTTP_204_NO_CONTENT) def perform_destroy(self, instance): instance.delete() 基于扩展类实现BookDetailView: 1234567891011121314class BookDetailView(GenericAPIView, RetrieveModelMixin, UpdateModelMixin, DestroyModelMixin): queryset = Book.objects.all() serializer_class = BookSerializer def get(self, request, pk): return self.retrieve(request, pk) def put(self, request, pk): return self.update(request, pk) def delete(self, request, pk): return self.destroy(request, pk) 整体代码，使用GenericAPIView结合视图扩展类，实现5个基本api接口，视图代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&quot;&quot;&quot;Create your views here.基于GenericAPIView结合5个视图扩展类完成基本的5个API接口ListModelMixin 提供了list方法，获取多条数据CreateModelMixin 提供了create方法，添加一条数据RetrieveModelMixin 提供了retrieve方法，获取一条数据UpdateModelMixin 提供了update方法，更新一条数据DestroyModelMixin 提供了destroy方法，删除一条数据&quot;&quot;&quot;&#x27;&#x27;&#x27;设计增删改查查接口&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;序列化器&#x27;&#x27;&#x27;from rest_framework import serializersfrom rest_framework.response import Responsefrom .models import Bookfrom rest_framework.generics import GenericAPIViewfrom rest_framework import statusfrom rest_framework.mixins import ListModelMixin, CreateModelMixin, RetrieveModelMixin, UpdateModelMixin, \\ DestroyModelMixinclass BookSerializer(serializers.ModelSerializer): &quot;&quot;&quot;图书数据序列化器&quot;&quot;&quot; class Meta: model = Book fields = &#x27;__all__&#x27;class BookView(GenericAPIView, ListModelMixin, CreateModelMixin): queryset = Book.objects.all() serializer_class = BookSerializer def get(self, request): return self.list(request) def post(self, request): return self.create(request)class BookDetailView(GenericAPIView, RetrieveModelMixin, UpdateModelMixin, DestroyModelMixin): queryset = Book.objects.all() serializer_class = BookSerializer def get(self, request, pk): return self.retrieve(request, pk) def put(self, request, pk): return self.update(request, pk) def delete(self, request, pk): return self.destroy(request, pk) 路由不变: 1234567urlpatterns = [ path(&#x27;books/&#x27;, BookView.as_view()), re_path(&#x27;books/(?P&lt;pk&gt;\\d+)/&#x27;, BookDetailView.as_view()),] 9.3、 GenericAPIView的视图子类（1）CreateAPIView提供了post方法，内部调用了create方法 继承自： GenericAPIView、CreateModelMixin （2）ListAPIView提供了get方法，内部调用了list方法 继承自：GenericAPIView、ListModelMixin （3）RetrieveAPIView提供了get方法，内部调用了retrieve方法 继承自: GenericAPIView、RetrieveModelMixin （4）DestoryAPIView提供了delete方法，内部调用了destory方法 继承自：GenericAPIView、DestoryModelMixin （5）UpdateAPIView提供了put和patch方法，内部调用了update和partial_update方法 继承自：GenericAPIView、UpdateModelMixin （6）ListCreateAPIView提供了get和post方法，内部调用了list和create方法 继承自：GenericAPIView、ListModelMixin、CreateModelMixin （7）RetrieveUpdateAPIView提供 get、put、patch方法 继承自： GenericAPIView、RetrieveModelMixin、UpdateModelMixin （8）RetrieveDestoryAPIView提供 get、delete方法 继承自：GenericAPIView、RetrieveModelMixin、DestoryModelMixin （9）RetrieveUpdateDestoryAPIView提供 get、put、patch、delete方法 继承自：GenericAPIView、RetrieveModelMixin、UpdateModelMixin、DestoryModelMixin 123456789101112131415161718192021&#x27;&#x27;&#x27;设计增删改查查接口&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;序列化器&#x27;&#x27;&#x27;from rest_framework import serializersfrom .models import Bookfrom rest_framework import genericsclass BookSerializer(serializers.ModelSerializer): &quot;&quot;&quot;图书数据序列化器&quot;&quot;&quot; class Meta: model = Book fields = &#x27;__all__&#x27;class BookView(generics.ListCreateAPIView): queryset = Book.objects.all() serializer_class = BookSerializerclass BookDetailView(generics.RetrieveUpdateDestroyAPIView): queryset = Book.objects.all() serializer_class = BookSerializer 9.4、视图集9.4.1、ViewSet继承自 APIView 与 ViewSetMixin , 作用也与APIView基本类似, 提供了身份认证、权限校验、流量管理等. **ViewSet主要通过继承ViewSetMixin来实现在调用as_view()时传入字典{“http请求”:”视图方法”}的映射处理工作, 如{‘get’:’list’}, ** 在ViewSet中, 没有提供任何动作action方法, 需要我们自己实现action方法. 使用视图集ViewSet, 可以将一系列视图相关的代码逻辑和相关的http请求动作封装到一个类中: list() 提供一组数据 retrieve() 提供单个数据 create() 创建数据 update() 保存数据 destory() 删除数据 ViewSet视图集类不再限制视图方法名只允许get()、post()等这种情况了, 而是实现允许开发者根据自己的需要定义自定义方法名, 例如 list() 、create() 等, 然后经过路由中使用http和这些视图方法名进行绑定调用. 为了方便演示视图集的使用, 我们新建一个子应用, 1python manage.py startapp vset 视图集只在使用as_view()方法的时候, 才会将action动作与具体请求方式对应上. 如: 12345678910111213141516171819from django.urls import path, re_pathfrom vset.views import BookViewurlpatterns = [ &#x27;&#x27;&#x27;path(&quot;set&quot;, views.BookView.as_view(&#123;&quot;http请求&quot;:&quot;视图方法&quot;&#125;)),&#x27;&#x27;&#x27; path(&quot;books/&quot;, BookView.as_view(&#123; &quot;get&quot;: &quot;get_all_book&quot;, &quot;post&quot;: &quot;add_book&quot; &#125;)), re_path(&quot;^books/(?P&lt;pk&gt;\\d+)$&quot;, BookView.as_view(&#123; &quot;get&quot;: &quot;get_one_book&quot;, &quot;put&quot;: &quot;edit_book&quot;, &quot;delete&quot;: &quot;delete&quot;, &#125;)),] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546from rest_framework import serializersfrom rest_framework import statusfrom rest_framework.response import Responsefrom rest_framework.viewsets import ViewSetfrom sers.models import Bookclass BookSerializer(serializers.ModelSerializer): class Meta: model = Book fields = &quot;__all__&quot;class BookView(ViewSet): def get_all_book(self, request): books = Book.objects.all() bs = BookSerializer(instance=books, many=True) return Response(bs.data) def add_book(self, request): bs = BookSerializer(data=request.data) if bs.is_valid(): bs.save() return Response(bs.data) else: return Response(bs.errors) def get_one_book(self, request, pk): book = Book.objects.get(pk=pk) bs = BookSerializer(instance=book) return Response(bs.data) def edit_book(self, request, pk): instance = Book.objects.get(pk=pk) bs = BookSerializer(instance=instance, data=request.data) if bs.is_valid(): bs.save() return Response(bs.data) else: return Response(bs.errors) def delete(self, request, pk): Book.objects.get(pk=pk).delete() return Response(status=status.HTTP_204_NO_CONTENT) 9.4.2、GenericViewSet继承自GenericAPIView和ViewSetMixin, 作用让视图集的视图代码变得更加通用, 抽离独特代码作为视图类的属性. 使用ViewSet通常并不方便, 因为list、retrieve、create、update、destory等方法都需要自己编写, 而这些方法与前面讲过的Mixin扩展类提供的方法同名, 所以我们可以通过继承Mixin扩展类来复用这些方法而无需自己编写. 但是Mixin扩展类依赖与 GenericAPIView , 所以还需要继承 GenericAPIView . GenericViewSet就帮助我们完成了这样的继承工作, 继承自 GenericAPIView 与 ViewSetMixin , 在实现了调用as_view()时传入字典(如 &#123;&#39;get&#39;:&#39;list&#39;&#125; )的映射处理工作的同时, 还提供了 GenericAPIView 提供的基础方法, 可以直接搭配Mixin扩展类使用. 视图代码: 123456789101112131415161718192021222324252627282930313233343536from rest_framework.viewsets import GenericViewSetclass BookView(GenericViewSet): def list(self, request): books = Book.objects.all() bs = BookSerializer(instance=books, many=True) return Response(bs.data) def create(self, request): bs = BookSerializer(data=request.data) if bs.is_valid(): bs.save() return Response(bs.data) else: return Response(bs.errors) def retrieve(self, request, pk): book = Book.objects.get(pk=pk) bs = BookSerializer(instance=book) return Response(bs.data) def update(self, request, pk): instance = Book.objects.get(pk=pk) bs = BookSerializer(instance=instance, data=request.data) if bs.is_valid(): bs.save() return Response(bs.data) else: return Response(bs.errors) def delete(self, request, pk): Book.objects.get(pk=pk).delete() return Response(status=status.HTTP_204_NO_CONTENT) 12345678910111213141516from django.urls import path, re_pathfrom vset.views import BookViewurlpatterns = [ &#x27;&#x27;&#x27;path(&quot;set&quot;, views.BookView.as_view(&#123;&quot;http请求&quot;:&quot;视图方法&quot;&#125;)),&#x27;&#x27;&#x27; path(&quot;books/&quot;, BookView.as_view(&#123; &quot;get&quot;: &quot;list&quot;, &quot;post&quot;: &quot;create&quot; &#125;)), re_path(&quot;^books/(?P&lt;pk&gt;\\d+)$&quot;, BookView.as_view(&#123; &quot;get&quot;: &quot;retrieve&quot;, &quot;put&quot;: &quot;update&quot;, &quot;delete&quot;: &quot;delete&quot;, &#125;)),] 集合我们上面学习的模型扩展类, 实现简写操作, 视图, 代码: 1234567891011from rest_framework.viewsets import GenericViewSetfrom rest_framework.mixins import ListModelMixin, CreateModelMixin, RetrieveModelMixin, UpdateModelMixin, \\ DestroyModelMixinclass BookView(GenericViewSet, ListModelMixin, CreateModelMixin, RetrieveModelMixin, UpdateModelMixin, DestroyModelMixin): queryset = Book.objects serializer_class = BookSerializer 9.4.3、ModelViewSet和ReadOnlyModelViewSetModelViewSet继承自GenericViewSet，同时包括了ListModelMixin、RetrieveModelMixin、CreateModelMixin、UpdateModelMixin、DestoryModelMixin。 ReadOnlyModelViewSet承自GenericViewSet，同时包括了ListModelMixin、RetrieveModelMixin。 12345from rest_framework.viewsets import ModelViewSetclass BookView(ModelViewSet): queryset = Book.objects serializer_class = BookSerializer 10. 路由Routers对于视图集ViewSet, 我们除了可以自己手动指明请求方式与动作action之间的对应关系外, 还可以使用Routers来帮助我们快速实现路由信息. REST framework提供了两个router SimpleRouter DefaultRouter 10.1 使用方法 创建router对象, 并注册视图集, 例如 123from rest_framework import routersrouter = routers.DefaultRouter()router.register(&#x27;book&#x27;, BookView, base_name=&#x27;book&#x27;) register(prefix, viewset, base_name) prefix 该视图集的路由前缀 viewset 视图集 base_name 路由别名的前缀 如上述代码会形成的路由如下: 12^book/$ name: book-list^book/&#123;pk&#125;/$ name: book-detail 2)添加路由数据 可以有两种方式: 12345urlpatterns = [ ...]urlpatterns += router.urls 或 12345urlpatterns = [ ... path(&#x27;^&#x27;, include(router.urls))] 路由代码: 12345678910111213141516171819from django.urls import path, re_pathfrom . import viewsurlpatterns = [ ...]&quot;&quot;&quot;使用drf提供路由类router给视图集生成路由列表&quot;&quot;&quot;&#x27;&#x27;&#x27;实例化路由类drf提供一共提供了两个路由类给我们使用,他们用法一致,功能几乎一样&#x27;&#x27;&#x27;from rest_framework.routers import DefaultRouterrouter = DefaultRouter()&#x27;&#x27;&#x27;注册视图集&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;router.register(&quot;路由前缀&quot;,视图集类)&#x27;&#x27;&#x27;router.register(&quot;book&quot;,views.BookView)&#x27;&#x27;&#x27;把生成的路由列表追加到urlpatterns&#x27;&#x27;&#x27;print( router.urls )urlpatterns += router.urls 上面的代码就成功生成了路由地址[增&#x2F;删&#x2F;改&#x2F;查一条&#x2F;查多条的功能], 但是不会自动我们在视图集自定义方法的路由. 所以我们如果也要给自定义方法生成路由, 则需要进行action动作的声明. 10.2 视图集中附加action的声明在视图集中, 如果想要让Router自动帮助我们为自定义的动作生成路由信息, 需要使用 rest_framework.decorators.action 装饰器. 以action装饰器装饰的方法名会作为action动作名, 与list、retrieve等同. action装饰器可以接收两个参数: methods: 声明该action对应的请求方式, 列表传递 detail: 声明该action的路径是否与单一资源对应 detail: 声明该action的路径是否与单一资源对应 detail: 声明该action的路径是否与单一资源对应 1路由前缀/&lt;pk&gt;/action方法名/ True 表示路径格式是xxx/&lt;pk&gt;/action方法名/ False 表示路径格式是xxx/action方法名/ url_path: 声明该action的路由尾缀. 举例: 12345678910111213141516171819202122232425from rest_framework.viewsets import ModelViewSetfrom rest_framework.decorators import actionclass BookView(ModelViewSet): queryset = Book.objects serializer_class = BookSerializer &quot;&quot;&quot; action装饰器的作用：告诉路由类给视图集的自定义方法生成路由信息 methods, 列表，允许哪些http请求能访问当前视图方法 detail，布尔，生成路由时是否拼接pk参数 detail为True，表示路径名格式应该为 book/&#123;pk&#125;/login/ url_path，字符串，生成路由时末尾路由路径，如果没有声明，则自动以当前方法名作为路由尾缀 &quot;&quot;&quot; @action(methods=[&#x27;get&#x27;], detail=True,url_path=&quot;login&quot;) def login(self, request,pk): &quot;&quot;&quot;登录&quot;&quot;&quot; return Response(&#123;&quot;msg&quot;:request.method&#125;) &#x27;&#x27;&#x27;detail为False 表示路径名格式应该为 book/get_new_5/&#x27;&#x27;&#x27; @action(methods=[&#x27;get&#x27;], detail=False) def get_new_5(self, request): &quot;&quot;&quot;获取最新添加的5本书&quot;&quot;&quot; ... 由路由器自动为此视图集自定义action方法形成的路由会是如下内容: 12^book/get_new_5/$ name: book-get_new_5^book/&#123;pk&#125;/login/$ name: book-login 10.3 路由router形成URL的方式 SimpleRouter(prefix&#x3D;”路由前缀”, viewset&#x3D;视图集类, basename&#x3D;”路由别名”) 2)DefaultRouter DefaultRouter与SimpleRouter的区别是, DefaultRouter会多附带一个默认的API根视图, 返回一个包含所有列表视图的超链接响应数据. 11、其它功能组件为了方便接下来的学习, 我们创建一个新的子应用 opt 1python manage.py startapp opt 注册子应用 1234INSTALLED_APPS = [ ... &#x27;opt&#x27;, # drf提供的组件使用] 总路由, 代码: 12345678910111213from django.contrib import adminfrom django.urls import path, includeurlpatterns = [ path(&#x27;admin/&#x27;, admin.site.urls), path(&#x27;students/&#x27;, include(&quot;students.urls&quot;)), path(&#x27;sers/&#x27;, include(&quot;sers.urls&quot;)), path(&#x27;school/&#x27;, include(&quot;school.urls&quot;)), path(&quot;req/&quot;, include(&quot;req.urls&quot;)), path(&quot;demo/&quot;, include(&quot;demo.urls&quot;)), path(&quot;opt/&quot;, include(&quot;opt.urls&quot;)),] 子路由, 代码: 12345from django.urls import pathfrom . import viewsurlpatterns = [] 因为接下来的认证组件中需要使用到登陆功能, 所以我们使用django内置admin站点并创建一个管理员. admin运营站点的访问地址:http://127.0.0.1:8000/admin 123python manage.py createsuperuser&#x27;&#x27;&#x27;如果之前有账号，但是忘了，可以通过终端下的命令修改指定用户的密码，这里的密码必须8位长度以上的&#x27;&#x27;&#x27;python manage.py changepassword 用户名 创建管理员以后, 访问admin站点, 先修改站点的语言配置 settings.py 123LANGUAGE_CODE = &#x27;zh-hans&#x27;TIME_ZONE = &#x27;Asia/Shanghai&#x27; 11.1. 认证Authentication可以在配置文件中配置全局默认的认证方案 常见的认证方式:cookie、session、token &#x2F;home&#x2F;moluo&#x2F;.virtualenvs&#x2F;drfdemo&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;rest_framework&#x2F;settings.py 默认配置文件 123456REST_FRAMEWORK = &#123; &#x27;DEFAULT_AUTHENTICATION_CLASSES&#x27;: ( &#x27;rest_framework.authentication.SessionAuthentication&#x27;, # session认证 &#x27;rest_framework.authentication.BasicAuthentication&#x27;, # 基本认证 )&#125; 也可以在具体的视图类中通过设置authentication_classess类属性来设置单独的不同的认证方式 1234567from rest_framework.authentication import SessionAuthentication, BasicAuthenticationfrom rest_framework.views import APIViewclass ExampleView(APIView): authentication_classes = [SessionAuthentication, BasicAuthentication] def get(self,request): pass 认证失败会有两种可能的返回值, 这个需要我们配合权限组件来使用: 401 Unauthorized 未认证 403 Permission Denied 权限被禁止 自定义认证, drfdemo.authentication 代码: 123456789101112131415161718192021from rest_framework.authentication import BaseAuthenticationfrom rest_framework.exceptions import APIExceptionclass CustomAuthentication(BaseAuthentication): &quot;&quot;&quot; 自定义认证方式 &quot;&quot;&quot; def authenticate(self, request): print(&quot;:::&quot;) &quot;&quot;&quot; 认证方法 request: 本次客户端发送过来的http请求对象 &quot;&quot;&quot; &#x27;&#x27;&#x27;token = request.query_params.get(&quot;token&quot;)&#x27;&#x27;&#x27; token = request._request.META.get(&quot;HTTP_TOKEN&quot;) if token != &quot;123456789&quot;: raise APIException(&quot;认证失败&quot;) user = &quot;root&quot; return (user,token) # 按照固定的返回格式填写 （用户模型对象, None） 视图调用自定义认证, 视图代码: 12345678910111213141516from django.contrib.auth.models import AnonymousUserfrom django.shortcuts import renderfrom rest_framework.views import APIViewfrom rest_framework.response import Responsefrom rest_framework.authentication import SessionAuthenticationfrom drfdemo.authentication import CustomAuthentication# Create your views here.class HomeAPIView(APIView): &#x27;&#x27;&#x27;authentication_classes = [CustomAuthentication, ]&#x27;&#x27;&#x27; def get(self,request): &quot;&quot;&quot;单独设置认证方式&quot;&quot;&quot; print(request.user) # 在中间件AuthenticationMiddleware中完成用户身份识别的，如果没有登录request.user值为AnonymousUser if request.user.id is None: return Response(&quot;未登录用户：游客&quot;) else: return Response(f&quot;已登录用户：&#123;request.user&#125;&quot;) 当然, 也可以注释掉上面视图中的配置, 改成全局配置.settings.py, 代码: 123456789&quot;&quot;&quot;drf配置信息必须全部写在REST_FRAMEWORK配置项中&quot;&quot;&quot;REST_FRAMEWORK = &#123; &#x27;&#x27;&#x27;配置认证方式的选项【drf的认证是内部循环遍历每一个注册的认证类，一旦认证通过识别到用户身份，则不会继续循环】&#x27;&#x27;&#x27; &#x27;DEFAULT_AUTHENTICATION_CLASSES&#x27;: ( &#x27;drfdemo.authentication.CustomAuthentication&#x27;, # 自定义认证 &#x27;rest_framework.authentication.SessionAuthentication&#x27;, # session认证 &#x27;rest_framework.authentication.BasicAuthentication&#x27;, # 基本认证 )&#125; 11.2. 权限Permissions权限控制可以限制用户对于视图的访问和对于具有模型对象的访问. 在执行视图的as_view()方法的dispatch()方法前, 会先进行视图访问权限的判断 在通过get_object()获取具体模型对象时, 会进行模型对象访问权限的判断 使用可以在配置文件中全局设置默认的权限管理类, 如 1234567REST_FRAMEWORK = &#123; .... &#x27;DEFAULT_PERMISSION_CLASSES&#x27;: ( &#x27;rest_framework.permissions.IsAuthenticated&#x27;, )&#125; 如果未指明, 则采用如下默认配置 123&#x27;DEFAULT_PERMISSION_CLASSES&#x27;: ( &#x27;rest_framework.permissions.AllowAny&#x27;,) 也可以在具体的视图中通过permission_classes属性来进行局部设置, 如 123456from rest_framework.permissions import IsAuthenticatedfrom rest_framework.views import APIViewclass ExampleView(APIView): permission_classes = (IsAuthenticated,) ... 提供的权限 AllowAny 允许所有用户, 默认权限 IsAuthenticated 仅通过登录认证的用户 IsAdminUser 仅管理员用户 IsAuthenticatedOrReadOnly 已经登陆认证的用户可以对数据进行增删改操作, 没有登陆认证的只能查看数据. 举例123456789from rest_framework.authentication import SessionAuthenticationfrom rest_framework.permissions import IsAuthenticatedfrom rest_framework.generics import RetrieveAPIViewclass VulnAPIView(RetrieveAPIView): queryset = Student.objects.all() serializer_class = VulnSerializer authentication_classes = [SessionAuthentication] permission_classes = [IsAuthenticated] 自定义权限如需自定义权限, 需继承rest_framework.permissions. BasePermission父类, 并实现以下两个任何一个方法或全部 .has_permission(self, request, view) 是否可以访问视图, view表示当前视图对象 .has_object_permission(self, request, view, obj) 是否可以访问模型对象, view表示当前视图, obj为模型数据对象 例如: 在当前子应用下, 创建一个权限文件drfdemo.permissions.py中声明自定义权限类: 123456789101112131415161718192021from rest_framework.permissions import BasePermissionclass IsXiaoMingPermission(BasePermission): &quot;&quot;&quot; 自定义权限，可用于全局配置，也可以用于局部 &quot;&quot;&quot; def has_permission(self, request, view): &quot;&quot;&quot; 视图权限 返回结果未True则表示允许访问视图类 request: 本次客户端提交的请求对象 view: 本次客户端访问的视图类 &quot;&quot;&quot; role = request.query_params.get(&quot;role&quot;) return role == &quot;xiaoming&quot; def has_object_permission(self, request, view, obj): &quot;&quot;&quot; 模型权限 返回结果为True则表示允许操作模型对象 &quot;&quot;&quot; return True 视图代码: 12345from .permissions import IsXiaoMingPermissionclass StudentViewSet(ModelViewSet): queryset = Student.objects.all() serializer_class = StudentSerializer permission_classes = [IsXiaoMingPermission] # 自定义权限 认证和权限的举例代码: settings.py, 全局配置, 代码: 1234567891011121314151617&#x27;&#x27;&#x27;关于REST_FRAMEWORK的所有配置项都是填写在django的settings配置文件中的。所有的REST_FRAMEWORK都要填写在 REST_FRAMEWORK的配置项，而且配置只能大写！！&#x27;&#x27;&#x27;REST_FRAMEWORK = &#123; &#x27;&#x27;&#x27;认证全局配置&#x27;&#x27;&#x27; &#x27;DEFAULT_AUTHENTICATION_CLASSES&#x27;:[ &#x27;&#x27;&#x27;默认由drf提供的认证方式&#x27;&#x27;&#x27; &#x27;rest_framework.authentication.SessionAuthentication&#x27;, # session认证 &#x27;rest_framework.authentication.BasicAuthentication&#x27;, # 基本认证 &#x27;&#x27;&#x27;将来开发中，我们还可以自己实现属于自己项目的认证方式&#x27;&#x27;&#x27; &#x27;drfdemo.authentications.CustomAuthentication&#x27;, ], &#x27;&#x27;&#x27;权限全局配置&#x27;&#x27;&#x27; &#x27;DEFAULT_PERMISSION_CLASSES&#x27;: [ &quot;&quot;&quot;设置所有视图只能被已经登录认证过的用户访问&quot;&quot;&quot; &#x27;rest_framework.permissions.IsAuthenticated&#x27;, ]&#125; 视图代码: 123456789101112131415161718from rest_framework.viewsets import ModelViewSetfrom student.models import Studentfrom student.serializers import StudentModelSerializerfrom drfdemo.authentications import CustomAuthenticationfrom rest_framework.authentication import SessionAuthenticationfrom rest_framework.permissions import IsAuthenticated,IsAdminUser,IsAuthenticatedOrReadOnlyfrom drfdemo.permissions import IsXiaoMingPermissionclass Student1ModelViewSet(ModelViewSet): queryset = Student.objects serializer_class = StudentModelSerializer&quot;&quot;&quot; # 局部认证配置方式&quot;&quot;&quot; authentication_classes = [SessionAuthentication,CustomAuthentication]&quot;&quot;&quot; # 局部权限配置方式 # permission_classes = [IsAuthenticated] # 只要经过认证登录就可以访问 # permission_classes = [IsAdminUser] # 只要是站点管理员就可以访问 # permission_classes = [IsAuthenticatedOrReadOnly] # 登录用户可以访问视图的增删查改页面，未登录的游客只能查看数据。不能修改！ # permission_classes = [] # 取消权限判断识别&quot;&quot;&quot; permission_classes = [IsXiaoMingPermission] # 自定义权限 自定义认证类, drfdemo.authentications , 代码: 123456789101112131415161718from rest_framework.authentication import SessionAuthentication,BaseAuthenticationfrom django.contrib.auth.models import Userclass CustomAuthentication(BaseAuthentication): &quot;&quot;&quot; 自定义认证 &quot;&quot;&quot; def authenticate(self, request): &quot;&quot;&quot; 认证方法 request: 本次客户端发送过来的http请求对象 &quot;&quot;&quot; role = request.query_params.get(&quot;role&quot;) root = None if role == &quot;root&quot;: root = User.objects.get(pk=1) return (root,None) # 按照固定的返回格式填写 （用户模型对象, None） else: return None 自定义权限类, defdemo.permissions , 代码: 123456789101112131415161718192021from rest_framework.permissions import BasePermissionclass IsXiaoMingPermission(BasePermission): &quot;&quot;&quot; 自定义权限，可用于全局配置，也可以用于局部 &quot;&quot;&quot; def has_permission(self, request, view): &quot;&quot;&quot; 视图权限 返回结果未True则表示允许访问视图类 request: 本次客户端提交的请求对象 view: 本次客户端访问的视图类 &quot;&quot;&quot; role = request.query_params.get(&quot;role&quot;) return role == &quot;xiaoming&quot; # 认证的结果必须返回True或者False，表示是否有权限 def has_object_permission(self, request, view, obj): &quot;&quot;&quot; 模型权限 返回结果未True则表示允许操作模型对象 &quot;&quot;&quot; return True urls, 路由代码: 12345678910from django.urls import path,includefrom . import viewsfrom rest_framework.routers import SimpleRouterrouter = SimpleRouter()router.register(&quot;stu1&quot;, views.Student1ModelViewSet,)urlpatterns = [ path(&quot;&quot;, include(router.urls)),] 11.3. 限流Throttling可以对接口访问的频次进行限制, 以减轻服务器压力, 或者实现特定的业务. 一般用于付费购买次数, 投票等场景使用. 基本使用可以在配置文件中, 使用 DEFAULT_THROTTLE_CLASSES 和 DEFAULT_THROTTLE_RATES 进行全局配置, 1234567891011REST_FRAMEWORK = &#123;&quot;&quot;&quot; # 限流全局配置 # &#x27;DEFAULT_THROTTLE_CLASSES&#x27;:[ # 限流配置类 # &#x27;rest_framework.throttling.AnonRateThrottle&#x27;, # 未认证用户[未登录用户] # &#x27;rest_framework.throttling.UserRateThrottle&#x27;, # 已认证用户[已登录用户] # ],&quot;&quot;&quot; &#x27;DEFAULT_THROTTLE_RATES&#x27;:&#123; # 频率配置 &#x27;anon&#x27;: &#x27;2/day&#x27;, # 针对游客的访问频率进行限制，实际上，drf只是识别首字母，但是为了提高代码的维护性，建议写完整单词 &#x27;user&#x27;: &#x27;5/day&#x27;, # 针对会员的访问频率进行限制， &#125;&#125; DEFAULT_THROTTLE_RATES 可以使用 second , minute , hour 或 day 来指明周期. 也可以在具体视图中通过throttle_classess属性来配置, 如 123456from rest_framework.throttling import UserRateThrottleclass Student2ModelViewSet(ModelViewSet): queryset = Student.objects serializer_class = StudentModelSerializer &quot;&quot;&quot;# 限流局部配置[这里需要配合在全局配置中的DEFAULT_THROTTLE_RATES来设置频率]&quot;&quot;&quot; throttle_classes = [UserRateThrottle] 可选限流类 AnonRateThrottle 限制所有匿名未认证用户, 使用IP区分用户.【很多公司这样的, IP结合设备信息来判断, 当然比IP要靠谱一点点而已】 使用 DEFAULT_THROTTLE_RATES[&#39;anon&#39;] 来设置频次 2)UserRateThrottle 限制认证用户, 使用User模型的 id主键 来区分. 使用 DEFAULT_THROTTLE_RATES[&#39;user&#39;] 来设置频次 3)ScopedRateThrottle 限制用户对于每个视图的访问频次, 使用ip或user id. settings.py, 代码: 12345678910111213REST_FRAMEWORK = &#123; &quot;&quot;&quot;# 限流全局配置&quot;&quot;&quot; &#x27;DEFAULT_THROTTLE_CLASSES&#x27;:[ # 限流配置类 &quot;&quot;&quot;# &#x27;rest_framework.throttling.AnonRateThrottle&#x27;, # 未认证用户[未登录用户] # &#x27;rest_framework.throttling.UserRateThrottle&#x27;, # 已认证用户[已登录用户]&quot;&quot;&quot; &#x27;rest_framework.throttling.ScopedRateThrottle&#x27;, # 自定义限流 ], &#x27;DEFAULT_THROTTLE_RATES&#x27;:&#123; # 频率配置 &#x27;anon&#x27;: &#x27;2/day&#x27;, # 针对游客的访问频率进行限制，实际上，drf只是识别首字母，但是为了提高代码的维护性，建议写完整单词 &#x27;user&#x27;: &#x27;5/day&#x27;, # 针对会员的访问频率进行限制， &#x27;vip&#x27;: &#x27;10/day&#x27;, # 针对会员的访问频率进行限制， &#125;&#125; 视图代码: 1234567from rest_framework.throttling import UserRateThrottleclass Student2ModelViewSet(ModelViewSet): queryset = Student.objects serializer_class = StudentModelSerializer &quot;&quot;&quot;# 限流局部配置[这里需要配合在全局配置中的DEFAULT_THROTTLE_RATES来设置频率] # throttle_classes = [UserRateThrottle] # 使用drf限流类来配置频率&quot;&quot;&quot; throttle_scope = &quot;vip&quot; # 自定义频率 11.4. 过滤Filtering对于列表数据可能需要根据字段进行过滤, 我们可以通过添加django-fitlter扩展来增强支持. 1pip install django-filter 在配置文件中增加过滤后端的设置: 123456789INSTALLED_APPS = [ ... &#x27;django_filters&#x27;, # 需要注册应用，]REST_FRAMEWORK = &#123; ... &#x27;DEFAULT_FILTER_BACKENDS&#x27;: (&#x27;django_filters.rest_framework.DjangoFilterBackend&#x27;,)&#125; 在视图类中添加类属性filter_fields, 指定可以过滤的字段 12345class StudentListView(ListAPIView): queryset = Student.objects.all() serializer_class = StudentSerializer filter_fields = [&#x27;age&#x27;, &#x27;sex&#x27;] 11.5. 排序Ordering对于列表数据, REST framework提供了OrderingFilter过滤器来帮助我们快速指明数据按照指定字段进行排序. 使用方法: 在类视图中设置filter_backends, 使用 rest_framework.filters.OrderingFilter 过滤器, REST framework会在请求的查询字符串参数中检查是否包含了ordering参数, 如果包含了ordering参数, 则按照ordering参数指明的排序字段对数据集进行排序. 前端可以传递的ordering参数的可选字段值需要在ordering_fields中指明. 示例: 123456789class StudentListView(ListAPIView): queryset = Student.objects.all() serializer_class = StudentModelSerializer filter_backends = [OrderingFilter] ordering_fields = [&#x27;id&#x27;, &#x27;age&#x27;]&quot;&quot;&quot;# 127.0.0.1:8000/books/?ordering=-age# -id 表示针对id字段进行倒序排序# id 表示针对id字段进行升序排序&quot;&quot;&quot; 如果需要在过滤以后再次进行排序, 则需要两者结合! 全局配置下的过滤组件不能和排序组件一起使用, 只支持局部配置的过滤组件和排序组件一起使用. 123456789101112from rest_framework.generics import ListAPIViewfrom students.models import Studentfrom .serializers import StudentModelSerializerfrom django_filters.rest_framework import DjangoFilterBackendclass Student3ListView(ListAPIView): queryset = Student.objects.all() serializer_class = StudentModelSerializer filter_fields = [&#x27;age&#x27;, &#x27;sex&#x27;] &quot;&quot;&quot;# 因为局部配置会覆盖全局配置,所以需要重新把过滤组件核心类再次声明, # 否则过滤功能会失效&quot;&quot;&quot; filter_backends = [OrderingFilter,DjangoFilterBackend] ordering_fields = [&#x27;id&#x27;, &#x27;age&#x27;] 11.6. 分页Pagination因为django默认提供的分页器主要使用于前后端不分离的业务场景, 所以REST framework也提供了分页的支持. 我们可以在配置文件中设置全局的分页方式, 如: 1234REST_FRAMEWORK = &#123; &#x27;DEFAULT_PAGINATION_CLASS&#x27;: &#x27;rest_framework.pagination.PageNumberPagination&#x27;, &#x27;PAGE_SIZE&#x27;: 100 # 每页数目&#125; 12345&quot;&quot;&quot;# 如果在配置settings.py文件中， 设置了全局分页，那么在drf中凡是调用了ListModelMixin的list()，都会自动分页。如果项目中出现大量需要分页的数据，只有少数部分的分页，则可以在少部分的视图类中关闭分页功能。# 另外，视图类在使用过分页以后，务必在编写queryset属性时，模型.objects后面调用结果。例如：# Student.objects.all()&quot;&quot;&quot;class Student3ModelViewSet(ListAPIView): pagination_class = None 也可通过自定义Pagination类, 来为视图添加不同分页行为. 在视图中通过 pagination_clas 属性来指明. 可选分页器: PageNumberPagination 前端访问网址形式: 1GET http://127.0.0.1:8000/students/?page=4 可以在子类中定义的属性: page_size 每页数目 page_query_param 前端发送的页数关键字名, 默认为”page” page_size_query_param 前端发送的每页数目关键字名, 默认为None max_page_size 前端最多能设置的每页数量 分页器类, paginations , 代码: 1234567from rest_framework.pagination import PageNumberPagination,LimitOffsetPaginationclass StudentPageNumberPagination(PageNumberPagination): page_query_param = &quot;page&quot; # 查询字符串中代表页码的变量名 page_size_query_param = &quot;size&quot; # 查询字符串中代表每一页数据的变量名 page_size = 2 # 每一页的数据量 max_page_size = 4 # 允许客户端通过查询字符串调整的最大单页数据量 视图, views , 代码: 12345678from .paginations import StudentPageNumberPagination,StudentLimitOffsetPaginationclass Student3ModelViewSet(ModelViewSet): queryset = Student.objects.all() serializer_class = StudentModelSerializer &quot;&quot;&quot;# 取消当前视图类的分页效果 # pagination_class = None # 局部分页&quot;&quot;&quot; pagination_class = StudentPageNumberPagination 2)LimitOffsetPagination 前端访问网址形式: 1GET http://127.0.0.1/four/students/?limit=100&amp;offset=100 可以在子类中定义的属性: default_limit 默认限制, 默认值与PAGE_SIZE设置一直 limit_query_param limit参数名, 默认’limit’ offset_query_param offset参数名, 默认’offset’ max_limit 最大limit限制, 默认None 分页类, 代码: 1234567from rest_framework.pagination import PageNumberPagination,LimitOffsetPaginationclass StudentLimitOffsetPagination(LimitOffsetPagination): limit_query_param = &quot;limit&quot; # 查询字符串中代表每一页数据的变量名 offset_query_param = &quot;offset&quot; # 查询字符串中代表页码的变量名 default_limit = 2 # 每一页的数据量 max_limit = 4 # 允许客户端通过查询字符串调整的最大单页数据量 视图, views , 代码: 12345from .paginations import StudentPageNumberPagination,StudentLimitOffsetPaginationclass Student3ModelViewSet(ModelViewSet): queryset = Student.objects.all() serializer_class = StudentModelSerializer pagination_class = StudentLimitOffsetPagination 11.7. 异常处理 ExceptionsREST framework提供了异常处理, 我们可以自定义异常处理函数. 例如我们想在要创建一个自定义异常函数, 这个函数, 我们保存到当前子应用opt中[注意, 开发时, 我们会找个独立的公共目录来保存这种公共的函数&#x2F;工具&#x2F;类库]. 1234567891011from rest_framework.views import exception_handlerdef custom_exception_handler(exc, context): &quot;&quot;&quot;# 先调用REST framework默认的异常处理方法获得标准错误响应对象&quot;&quot;&quot; response = exception_handler(exc, context) &quot;&quot;&quot;# 在此处补充自定义的异常处理&quot;&quot;&quot; if response is None: response.data[&#x27;status_code&#x27;] = response.status_code return response 在配置文件中声明自定义的异常处理, settings , 代码: 123REST_FRAMEWORK = &#123; &#x27;EXCEPTION_HANDLER&#x27;: &#x27;drfdemo.exceptions.custom_excetion_handle&#x27;&#125; 如果未声明, 会采用默认的方式, 如下 rest_frame&#x2F;settings.py 123REST_FRAMEWORK = &#123; &#x27;EXCEPTION_HANDLER&#x27;: &#x27;rest_framework.views.exception_handler&#x27;&#125; 例如: 补充上处理关于数据库的异常, 这里使用其他异常来举例: 主应用.exceptions , 代码: 123456789101112131415161718192021222324252627282930&quot;&quot;&quot;# 自定义异常函数: 在drf本身提供的异常函数基础上，我们增加更多的异常处理就可以了。&quot;&quot;&quot;from rest_framework.views import exception_handlerfrom django.db import DatabaseErrorfrom rest_framework import statusfrom rest_framework.response import Responsedef custom_excetion_handle(exc, context): &quot;&quot;&quot; 自定义异常函数，必须要在配置文件中注册才能被drf使用 exc: 异常对象，本次发生的异常对象 context: 字典，本次发生异常时，python解析器提供的执行上下文 所谓的执行上下文[context]，就是程序执行到当前一行代码时，能提供给开发者调用的环境信息异常发生时，代码所在的路径，时间，视图，客户端http请求等等...] &quot;&quot;&quot; &quot;&quot;&quot;# 先让drf处理它能识别的异常&quot;&quot;&quot; response = exception_handler(exc, context) &quot;&quot;&quot;# 在经过了drf的异常处理以后，还是返回None则表示有2种情况:&quot;&quot;&quot; if response is None: &quot;&quot;&quot;# 异常发生时的视图对象&quot;&quot;&quot; view = context[&#x27;view&#x27;] &quot;&quot;&quot;# 异常发生时的http请求&quot;&quot;&quot; request = context[&quot;request&quot;] if isinstance(exc, DatabaseError): print(&#x27;[%s]: %s&#x27; % (view, exc)) response = Response(&#123;&#x27;detail&#x27;: &#x27;服务器内部错误&#x27;&#125;, status=status.HTTP_507_INSUFFICIENT_STORAGE) if isinstance(exc, TypeError): print(&quot;0不能作为除数~&quot;) print(request) response = Response(&#123;&#x27;detail&#x27;: &#x27;0不能作为除数&#x27;&#125;, status=status.HTTP_500_INTERNAL_SERVER_ERROR) return response 视图中, 故意报错: 12345from .paginations import StudentPageNumberPagination,StudentLimitOffsetPaginationclass Student3ModelViewSet(ModelViewSet): queryset = Student.objects # 去掉 .all()，就会报错。 serializer_class = StudentModelSerializer pagination_class = StudentPageNumberPagination REST framework定义的异常 APIException 所有异常的父类 ParseError 解析错误 AuthenticationFailed 认证失败 NotAuthenticated 尚未认证 PermissionDenied 权限决绝 NotFound 未找到 MethodNotAllowed 请求方式不支持 NotAcceptable 要获取的数据格式不支持 Throttled 超过限流次数 ValidationError 校验失败 也就是说, 很多的没有在上面列出来的异常, 就需要我们在自定义异常中自己处理了. 11.8. 自动生成接口文档REST framework可以自动帮助我们生成接口文档. 接口文档以网页的方式呈现. 自动接口文档能生成的是继承自 APIView 及其子类的视图. 11.8.1. 安装依赖REST framewrok生成接口文档需要 coreapi 库的支持. 1pip install coreapi 11.8.2. 设置接口文档访问路径在总路由中添加接口文档路径. 文档路由对应的视图配置为 rest_framework.documentation.include_docs_urls , 参数 title 为接口文档网站的标题. 总路由, 代码: 123456from rest_framework.documentation import include_docs_urlsurlpatterns = [ ... path(&#x27;docs/&#x27;, include_docs_urls(title=&#x27;站点页面标题&#x27;))] 在settings.py中配置接口文档. 123REST_FRAMEWORK = &#123; &#x27;DEFAULT_SCHEMA_CLASS&#x27;: &#x27;rest_framework.schemas.AutoSchema&#x27;,&#125; 11.8.3. 文档描述说明的定义位置 单一方法的视图, 可直接使用类视图的文档字符串, 如 1234class BookListView(generics.ListAPIView): &quot;&quot;&quot; 返回所有图书信息. &quot;&quot;&quot; 2)包含多个方法的视图, 在类视图的文档字符串中, 分开方法定义, 如 12345678class BookListCreateView(generics.ListCreateAPIView): &quot;&quot;&quot; get: 返回所有图书信息. post: 新建图书. &quot;&quot;&quot; 3)对于视图集ViewSet, 仍在类视图的文档字符串中封开定义, 但是应使用action名称区分, 如 1234567891011121314class BookInfoViewSet(mixins.ListModelMixin, mixins.RetrieveModelMixin, GenericViewSet): &quot;&quot;&quot; list: 返回图书列表数据 retrieve: 返回图书详情数据 latest: 返回最新的图书数据 read: 修改图书的阅读量 &quot;&quot;&quot; 11.8.4. 访问接口文档网页浏览器访问 127.0.0.1:8000&#x2F;docs&#x2F;, 即可看到自动生成的接口文档. 两点说明: 视图集ViewSet中的retrieve名称, 在接口文档网站中叫做read 2)参数的Description需要在模型类或序列化器类的字段中以help_text选项定义, 如: 1234class Student(models.Model): ... age = models.IntegerField(default=0, verbose_name=&#x27;年龄&#x27;, help_text=&#x27;年龄&#x27;) ... 或 12345678910class StudentSerializer(serializers.ModelSerializer): class Meta: model = Student fields = &quot;__all__&quot; extra_kwargs = &#123; &#x27;age&#x27;: &#123; &#x27;required&#x27;: True, &#x27;help_text&#x27;: &#x27;年龄&#x27; &#125; &#125;","categories":[{"name":"技术","slug":"技术","permalink":"https://champion-yang.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://champion-yang.github.io/tags/Django/"}]},{"title":"hexo建站","slug":"hexo建站","date":"2024-06-28T06:38:28.000Z","updated":"2024-07-04T08:41:10.865Z","comments":true,"path":"2024/06/28/hexo建站/","permalink":"https://champion-yang.github.io/2024/06/28/hexo%E5%BB%BA%E7%AB%99/","excerpt":"","text":"hexo 操作 创建项目 启动调试 创建文章 markdown必要操作 设置目录 插入图片 设置主题pure 部署推送到github hexo 操作创建项目123hexo init snail_bookcd snail_booknpm install 启动调试123hexo cleanhexo ghexo s 创建文章12hexo new &#x27;postName&#x27;hexo n -p 01_pandas/2024-07-04-pandas03 markdown必要操作设置目录vscode 安装插件 Markdown Preview Enhanced 你可以通过 cmd/ctrl-shift-p 然后选择 Markdown Preview Enhanced: Create Toc 命令来创建 TOC. 可查阅如下地址: https://www.bookstack.cn/read/mpe/zh-cn-toc.md 需要注意: 需要打开预览后, 再进行 ctrl-s 保持 md 文件, 才会生成目录在 当前 md 文件中. 插入图片推荐一种方式, 在 images 下, 根据知识库的 tags 进行目录创建 设置主题purepure主题就是我当前用的主题, 还可以 部署推送到github1hexo g -d","categories":[{"name":"工具","slug":"工具","permalink":"https://champion-yang.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://champion-yang.github.io/tags/hexo/"}]}],"categories":[{"name":"软考","slug":"软考","permalink":"https://champion-yang.github.io/categories/%E8%BD%AF%E8%80%83/"},{"name":"中项","slug":"软考/中项","permalink":"https://champion-yang.github.io/categories/%E8%BD%AF%E8%80%83/%E4%B8%AD%E9%A1%B9/"},{"name":"大数据","slug":"大数据","permalink":"https://champion-yang.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"存储","slug":"大数据/存储","permalink":"https://champion-yang.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AD%98%E5%82%A8/"},{"name":"技术","slug":"技术","permalink":"https://champion-yang.github.io/categories/%E6%8A%80%E6%9C%AF/"},{"name":"编程语言","slug":"编程语言","permalink":"https://champion-yang.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"数据科学","slug":"数据科学","permalink":"https://champion-yang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"},{"name":"数据处理","slug":"数据科学/数据处理","permalink":"https://champion-yang.github.io/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"},{"name":"系统架构师","slug":"系统架构师","permalink":"https://champion-yang.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"},{"name":"读书","slug":"读书","permalink":"https://champion-yang.github.io/categories/%E8%AF%BB%E4%B9%A6/"},{"name":"工具","slug":"工具","permalink":"https://champion-yang.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"软考","slug":"软考","permalink":"https://champion-yang.github.io/tags/%E8%BD%AF%E8%80%83/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://champion-yang.github.io/tags/Elasticsearch/"},{"name":"Kafka","slug":"Kafka","permalink":"https://champion-yang.github.io/tags/Kafka/"},{"name":"Python","slug":"Python","permalink":"https://champion-yang.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://champion-yang.github.io/tags/Pandas/"},{"name":"架构","slug":"架构","permalink":"https://champion-yang.github.io/tags/%E6%9E%B6%E6%9E%84/"},{"name":"书籍","slug":"书籍","permalink":"https://champion-yang.github.io/tags/%E4%B9%A6%E7%B1%8D/"},{"name":"Docker","slug":"Docker","permalink":"https://champion-yang.github.io/tags/Docker/"},{"name":"Redis","slug":"Redis","permalink":"https://champion-yang.github.io/tags/Redis/"},{"name":"Django","slug":"Django","permalink":"https://champion-yang.github.io/tags/Django/"},{"name":"hexo","slug":"hexo","permalink":"https://champion-yang.github.io/tags/hexo/"}]}